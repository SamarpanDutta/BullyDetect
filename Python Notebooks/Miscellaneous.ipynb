{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec model\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6580"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/clean_dataset.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique words in the dataset\n",
    "unique_words = []\n",
    "\n",
    "for comment in df['Comment']:\n",
    "    \n",
    "    for word in comment.split():\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF WORDS NOT IN MODEL:  1141\n",
      "PERCENTAGE MISSING:  5.916208648760759\n"
     ]
    }
   ],
   "source": [
    "# Check how many are not in the reddit model and collect them\n",
    "not_in_model = []\n",
    "\n",
    "for word in unique_words:\n",
    "    \n",
    "    if word not in model:\n",
    "        not_in_model.append(word)\n",
    "        \n",
    "print(\"NUMBER OF WORDS NOT IN MODEL: \", len(not_in_model))\n",
    "print(\"PERCENTAGE MISSING: \", (len(not_in_model)/len(unique_words)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the words then print them. Then, write them in a text file (DONE)\n",
    "#not_in_model = sorted(not_in_model)\n",
    "#with open('Missing words.txt', 'w') as fh:\n",
    "#    for word in not_in_model:\n",
    "#        fh.write(\"{}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146604\n"
     ]
    }
   ],
   "source": [
    "# Get the number of words in the Word2Vec model\n",
    "print(len(model.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arsehole', 0.8830243349075317),\n",
       " ('asshat', 0.8711205124855042),\n",
       " ('prick', 0.8389500975608826),\n",
       " ('dickhead', 0.8373932242393494),\n",
       " ('douchebag', 0.8241000175476074),\n",
       " ('dickbag', 0.8229827880859375),\n",
       " ('ahole', 0.8007912039756775),\n",
       " ('idiot', 0.7848369479179382),\n",
       " ('jackass', 0.7815600037574768),\n",
       " ('douche', 0.7751066088676453)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that are similar. This returns tuples in a list\n",
    "word = 'asshole'\n",
    "top_n = 10\n",
    "\n",
    "similar_words = model.most_similar(word, topn=top_n)\n",
    "model.most_similar(word, topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the Mean Cosine similarity among words\n",
    "import numpy as np\n",
    "\n",
    "mean_cos_distance = np.mean([ cos_distance for word, cos_distance in similar_words ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the collected words that are similar above this score\n",
    "words_above_mean = [word for word, cos_distance in similar_words if cos_distance > mean_cos_distance]\n",
    "total_words = float(len(words_above_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arsehole', 'asshat', 'prick', 'dickhead', 'douchebag', 'dickbag']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_above_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-initialize an empty numpy array (for speed)\n",
    "avgWordsFeature = np.zeros((300,),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over each word\n",
    "for word in words_above_mean:\n",
    "\n",
    "    # Add the word's vector\n",
    "    avgWordsFeature = np.add(avgWordsFeature,model[word])\n",
    "    \n",
    "# Average them out\n",
    "avgWordsFeature = np.divide(avgWordsFeature,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Use a numpy array to store each word as a feature\n",
    "# Use the first N-words. N is an integer\n",
    "\n",
    "sentenceWordFeature = np.zeros((5,), dtype=\"float32\")\n",
    "print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00470508  0.          0.          0.          0.        ]\n",
      "[-0.00470508  0.00158833  0.          0.          0.        ]\n",
      "[-0.00470508  0.00158833 -0.00082337  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Test with a single sentence\n",
    "\n",
    "sentence = \"you goddamn bastard\"\n",
    "complete_sentence = 1 # 1 for full, 0 for incomplete\n",
    "\n",
    "for i,word in enumerate(sentence.split()):\n",
    "    \n",
    "    if i == len(sentenceWordFeature):\n",
    "        complete_sentence = 0\n",
    "        break\n",
    "        \n",
    "    if word in model:\n",
    "        word_feature = np.mean(model[word])\n",
    "    else:\n",
    "        word_feature = -1.0\n",
    "        \n",
    "    sentenceWordFeature[i] = word_feature\n",
    "    \n",
    "    print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Soundex Dictionary (Normalized)\n",
    "soundex_dictionary = { \n",
    "    'B': \"1\", \"F\": \"1\", \"P\": \"1\",\n",
    "    \"V\": \"1\", \"C\": \"2\", \"G\": \"2\",\n",
    "    \"J\": \"2\", \"K\": \"2\", \"Q\": \"2\",\n",
    "    \"S\": \"2\", \"X\": \"2\", \"Z\": \"2\", \n",
    "    \"D\": \"3\", \"T\": \"3\", \"L\": \"4\", \n",
    "    \"M\": \"5\", \"N\": \"5\", \"R\": \"6\", \n",
    "    \"A\": \".\", \"E\": \".\", \"I\": \".\",\n",
    "    \"O\": \".\", \"U\": \".\", \"Y\": \".\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def getSoundex(word):\n",
    "    \n",
    "    # Uppercase the word\n",
    "    word = word.upper()\n",
    "\n",
    "    # Get the first letter of the word\n",
    "    soundex = word[0]\n",
    "\n",
    "    # Skip the following letters\n",
    "    skip_dict = \"HW\"\n",
    "    word = [letter for letter in word[1:] if letter not in skip_dict]\n",
    "    word = \"\".join(word)\n",
    "\n",
    "    # Loop character by character (Start with 2nd character)\n",
    "    for char in word[0:]:\n",
    "\n",
    "        code = soundex_dictionary[char]\n",
    "\n",
    "        if code != soundex[-1]:\n",
    "            soundex += code\n",
    "\n",
    "    # Replace period characters\n",
    "    soundex = soundex.replace(\".\", \"\")\n",
    "\n",
    "    # If the string has only one character, append rest with three 0s.\n",
    "    soundex = soundex[:4].ljust(4, \"0\")\n",
    "\n",
    "    return soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F230'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSoundex('fucked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Soundex dictionary\n",
    "import pickle\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/soundex_dict.pk\"\n",
    "\n",
    "soundex_dict = pickle.load(open(FILE,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL WORDS: 1146604\n",
      "UNIQUE SOUNDEX VALUES: 6599\n",
      "PERCENTAGE:  0.5755256391919092\n"
     ]
    }
   ],
   "source": [
    "# Perform operations here\n",
    "print(\"TOTAL WORDS: %i\" % (len(soundex_dict)))\n",
    "\n",
    "unique_soundex = list(set([value for key, value in soundex_dict.items()]))\n",
    "\n",
    "print(\"UNIQUE SOUNDEX VALUES: %i\" % (len(unique_soundex)))\n",
    "\n",
    "print(\"PERCENTAGE: \", (len(unique_soundex)/len(soundex_dict)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in missing word text file\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Text Files/Missing words.txt\"\n",
    "missing_words = []\n",
    "\n",
    "with open(FILE, 'r') as fh:\n",
    "    \n",
    "    # Iterate line by line\n",
    "    # Remove new line characters\n",
    "    for line in fh:\n",
    "        missing_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILAR WORDS FOUND: 1141\n"
     ]
    }
   ],
   "source": [
    "# Count how many missing words are in the soundex category\n",
    "count = 0\n",
    "\n",
    "for word in missing_words:\n",
    "    \n",
    "    soundex_encode = getSoundex(word)\n",
    "    \n",
    "    if soundex_encode in unique_soundex:\n",
    "        count += 1\n",
    "        \n",
    "print(\"SIMILAR WORDS FOUND: %i\" % (count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "# Load the transformed file\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/soundex_words_list.pk\"\n",
    "\n",
    "soundex_words_list = pickle.load(open(FILE,\"rb\"))\n",
    "\n",
    "soundex = getSoundex('fuckingloser')\n",
    "word_list = soundex_words_list[soundex]\n",
    "\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE LENGTH:  8.55740578439965\n"
     ]
    }
   ],
   "source": [
    "# Find average length of a missing word\n",
    "\n",
    "word_lengths = [len(word) for word in missing_words]\n",
    "\n",
    "print(\"AVERAGE LENGTH: \", sum(word_lengths)/len(missing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abuse abuse\n",
      "acidd acidd\n",
      "ackno acknowledge\n",
      "adole adolescent\n",
      "adumb adumb\n",
      "afroc afrocentric\n",
      "aidr aidra\n",
      "ajaja ajaja\n",
      "ajue ajuever\n",
      "aktu aktuell\n",
      "akunt akunt\n",
      "alber alberta\n",
      "aldas aldas\n",
      "alegi alegi\n",
      "aleja alejandro\n",
      "alist alistar\n",
      "aljaz aljazeera\n",
      "allah allah\n",
      "allin allin\n",
      "allra allrandom\n",
      "almaj almajid\n",
      "ameli amelia\n",
      "ameri american\n",
      "ameri american\n",
      "ammed ammed\n",
      "among among\n",
      "anara anarachy\n",
      "annag annagudbjorg\n",
      "annav annaversary\n",
      "annel anneliese\n",
      "anoel anoelr\n",
      "anony anonymous\n",
      "antag antagonist\n",
      "anton antonio\n",
      "antun antunes\n",
      "apple apple\n",
      "arbet arbeta\n",
      "arbit arbitrary\n",
      "areal areal\n",
      "argua arguably\n",
      "aritc aritcle\n",
      "arrno arrnold\n",
      "arrog arrogant\n",
      "artic article\n",
      "asper aspergers\n",
      "assho asshole\n",
      "assis assistance\n",
      "astro astronaut\n",
      "aunty aunty\n",
      "autob autobiography\n",
      "auxil auxiliary\n",
      "avand avand\n",
      "axles axles\n",
      "babas babas\n",
      "badha badham\n",
      "bagun bagunar\n",
      "baroc baroclinic\n",
      "beare bearer\n",
      "beaut beautiful\n",
      "becke becker\n",
      "beean beeans\n",
      "beebl beeblebrox\n",
      "belle belle\n",
      "belli belligerent\n",
      "beln belnd\n",
      "berli berlin\n",
      "berli berlin\n",
      "berth berth\n",
      "besit besitos\n",
      "besle besler\n",
      "beslu besluit\n",
      "beslu besluit\n",
      "beslu besluit\n",
      "betty betty\n",
      "biack biack\n",
      "bigst bigstory\n",
      "billy billy\n",
      "bitsk bitskeptic\n",
      "blago blagojevich\n",
      "blagu blague\n",
      "bless bless\n",
      "blitz blitz\n",
      "blogf blogfinger\n",
      "blood blood\n",
      "boeti boetius\n",
      "boisv boisvert\n",
      "bomit bomit\n",
      "borde border\n",
      "boyci boycie\n",
      "boyho boyhood\n",
      "brave brave\n",
      "brian brian\n",
      "bruse bruse\n",
      "bstru bstruct\n",
      "budwe budweiser\n",
      "buess buesseler\n",
      "bulls bullshit\n",
      "bushf bushfires\n",
      "buttf buttfuck\n",
      "buwha buwhahaha\n",
      "bwhah bwhahaha\n",
      "bytch bytch\n",
      "callt callthebluff\n",
      "camer camera\n",
      "camik camikaze\n",
      "canta cantaloupe\n",
      "carra carrack\n",
      "casso cassowary\n",
      "caust caustic\n",
      "cblog cblog\n",
      "celeb celebrate\n",
      "certi certified\n",
      "chard chard\n",
      "charl charlie\n",
      "chemi chemistry\n",
      "chewy chewy\n",
      "chiac chiac\n",
      "chiko chikorita\n",
      "chitt chitty\n",
      "choad choad\n",
      "chris christian\n",
      "chris christian\n",
      "chris christian\n",
      "chuck chuck\n",
      "chusi chusing\n",
      "cianf cianfrance\n",
      "civil civil\n",
      "clarr clarrett\n",
      "claud claude\n",
      "cockn cockney\n",
      "cockn cockney\n",
      "cocks cocks\n",
      "color color\n",
      "colro colros\n",
      "comfr comfrey\n",
      "comme comments\n",
      "commu community\n",
      "condi conditions\n",
      "consc conscious\n",
      "conta contact\n",
      "corpe corperate\n",
      "corre correct\n",
      "costa costa\n",
      "courc cource\n",
      "court court\n",
      "crapk crapkin\n",
      "creme creme\n",
      "crete crete\n",
      "crimp crimp\n",
      "criss criss\n",
      "crose crose\n",
      "cross cross\n",
      "ctown ctown\n",
      "cunni cunning\n",
      "curzi curzio\n",
      "cyber cyber\n",
      "cyndy cyndy\n",
      "dance dance\n",
      "davey davey\n",
      "debto debtor\n",
      "defea defeat\n",
      "defen defense\n",
      "delud deluded\n",
      "delus delusional\n",
      "delvo delvon\n",
      "demer demerit\n",
      "demok demoknight\n",
      "demok demoknight\n",
      "demon demon\n",
      "demon demon\n",
      "demta demtard\n",
      "derat derate\n",
      "desca descartes\n",
      "desig design\n",
      "detal detalii\n",
      "dhela dhelan\n",
      "diabt diabtes\n",
      "dickw dickwad\n",
      "diffi difficult\n",
      "digra digra\n",
      "dimpa dimpa\n",
      "dipsh dipshit\n",
      "disin disingenuous\n",
      "displ display\n",
      "disti distinction\n",
      "divlj divlji\n",
      "dmall dmall\n",
      "doobi doobie\n",
      "doouc doouche\n",
      "doppe doppelganger\n",
      "drrop drrop\n",
      "dsla dslam\n",
      "dumba dumbass\n",
      "dumba dumbass\n",
      "dumbg dumbgecko\n",
      "dumbp dumbphone\n",
      "dumf dumfries\n",
      "duxbu duxbury\n",
      "dycki dyckia\n",
      "dykes dykes\n",
      "dzie dziekanski\n",
      "easyr easyrider\n",
      "eatsh eatshit\n",
      "ecola ecolab\n",
      "econo economy\n",
      "ecot ecotools\n",
      "eduar eduardo\n",
      "einsp einspieler\n",
      "elaph elaphe\n",
      "emita emita\n",
      "encam encampment\n",
      "engla england\n",
      "etnik etnik\n",
      "eugen eugene\n",
      "evano evanovich\n",
      "everk everki\n",
      "faagg faaggoott\n",
      "fabre fabregas\n",
      "facun facundo\n",
      "fallo fallout\n",
      "farra farrah\n",
      "favor favorite\n",
      "feder federal\n",
      "feige feige\n",
      "feigl feiglin\n",
      "ferti fertility\n",
      "ffuuc ffuucckk\n",
      "filde filder\n",
      "filoz filozofie\n",
      "fireb fireball\n",
      "flaan flaan\n",
      "fleab fleabay\n",
      "fleab fleabay\n",
      "fleeb fleebnork\n",
      "flori florida\n",
      "flush flush\n",
      "folkv folkvangr\n",
      "fourg fourgy\n",
      "foxfa foxface\n",
      "foxta foxtail\n",
      "foxta foxtail\n",
      "freaa freaaking\n",
      "freec freecycle\n",
      "freed freedom\n",
      "frees freestyle\n",
      "frees freestyle\n",
      "frikk frikkin\n",
      "frrie frriend\n",
      "fucki fucking\n",
      "fucki fucking\n",
      "fucki fucking\n",
      "fucki fucking\n",
      "fucki fucking\n",
      "fuckk fuckk\n",
      "fuckl fuckload\n",
      "fucko fuckoff\n",
      "fucks fucks\n",
      "fuckt fuckton\n",
      "fuckw fuckwit\n",
      "fudge fudge\n",
      "gagge gagged\n",
      "galam galam\n",
      "galav galavant\n",
      "garup garupan\n",
      "gaybr gaybros\n",
      "gelug gelug\n",
      "genet genetic\n",
      "genom genome\n",
      "genui genuinely\n",
      "gessh gesshin\n",
      "gezu gezus\n",
      "ghost ghost\n",
      "gibbs gibbs\n",
      "giggl giggles\n",
      "gingr gingrich\n",
      "gispe gispert\n",
      "giudi giudice\n",
      "gjen gjendesheim\n",
      "glea glean\n",
      "glenn glenn\n",
      "gljiv gljive\n",
      "gnau gnaural\n",
      "gobie gobierno\n",
      "godwh godwhale\n",
      "golda golda\n",
      "golly golly\n",
      "gonst gonstead\n",
      "gonzo gonzo\n",
      "gopar goparokko\n",
      "gopro gopro\n",
      "gover government\n",
      "gramm grammar\n",
      "grazi grazing\n",
      "grich grichuk\n",
      "gropp gropping\n",
      "growa growable\n",
      "grund grundy\n",
      "gungo gungor\n",
      "gunja gunjack\n",
      "gurne gurney\n",
      "hahah hahaha\n",
      "hahah hahaha\n",
      "haled haledon\n",
      "hamme hammer\n",
      "hanni hannibal\n",
      "hanni hannibal\n",
      "happy happy\n",
      "hater haters\n",
      "hatet hatetrain\n",
      "helic helicopter\n",
      "helps helps\n",
      "hembe hembery\n",
      "heyro heyron\n",
      "hoida hoidays\n",
      "holdy holdy\n",
      "homot homotopy\n",
      "honem honemeister\n",
      "hoone hooned\n",
      "hopea hopeand\n",
      "houle houle\n",
      "huffn huffn\n",
      "hugu huguenots\n",
      "human human\n",
      "husar husaria\n",
      "ialis ialis\n",
      "ialis ialis\n",
      "iammo iammoose\n",
      "ignor ignore\n",
      "illne illness\n",
      "imore imore\n",
      "imper imperial\n",
      "incat incat\n",
      "incli inclined\n",
      "indep independent\n",
      "indid indidual\n",
      "indro indro\n",
      "indro indro\n",
      "inec inecraft\n",
      "infla inflation\n",
      "infli inflict\n",
      "infog infographic\n",
      "injoy injoy\n",
      "inneb innebunit\n",
      "insaa insaane\n",
      "insat insatiable\n",
      "insec insecure\n",
      "insic insicure\n",
      "inspi inspired\n",
      "insti institution\n",
      "insun insuniating\n",
      "intal intalnit\n",
      "intal intalnit\n",
      "isali isalith\n",
      "ishee isheep\n",
      "jaemi jaemis\n",
      "jardi jardine\n",
      "jasam jasamaha\n",
      "jayma jayman\n",
      "jeffr jeffrey\n",
      "jenku jenkum\n",
      "jepar jepardy\n",
      "jimdo jimdo\n",
      "jimmy jimmy\n",
      "jimmy jimmy\n",
      "joann joanna\n",
      "joeap joeap\n",
      "joelw joelwhyrock\n",
      "josey josey\n",
      "jumps jumps\n",
      "jungw jungwha\n",
      "justt justt\n",
      "kandi kandi\n",
      "karch karchev\n",
      "karlo karlos\n",
      "katsa katsap\n",
      "kenge kengen\n",
      "ketch ketchup\n",
      "ketta ketta\n",
      "khand khand\n",
      "kiffa kiffar\n",
      "kinuk kinukuha\n",
      "klane klane\n",
      "klass klassic\n",
      "klein klein\n",
      "kondo kondogbia\n",
      "konse konservative\n",
      "kontr kontrol\n",
      "korpi korpiklaani\n",
      "korpi korpiklaani\n",
      "korpi korpiklaani\n",
      "korth korth\n",
      "kovac kovacic\n",
      "kredi kredit\n",
      "kriti kritik\n",
      "kstu kstudioone\n",
      "ktore ktore\n",
      "kunth kunthunter\n",
      "kutv kutvolbraaksel\n",
      "label label\n",
      "lacc laccaria\n",
      "laiss laissez\n",
      "lambe lambert\n",
      "langc langcang\n",
      "latou latour\n",
      "laugi lauging\n",
      "lawma lawmakers\n",
      "layin laying\n",
      "lders lders\n",
      "leech leech\n",
      "lefta leftard\n",
      "leidi leiding\n",
      "lekha lekha\n",
      "lemon lemon\n",
      "libat libations\n",
      "libid libido\n",
      "libtu libturd\n",
      "libu libusb\n",
      "lifea lifeasan\n",
      "likew likewise\n",
      "litic litics\n",
      "littl little\n",
      "littl little\n",
      "lizza lizzard\n",
      "ljuds ljudska\n",
      "llade llade\n",
      "lnfao lnfao\n",
      "lolol lolol\n",
      "loves loves\n",
      "lowee lowee\n",
      "lubia lubiam\n",
      "lutem lutemis\n",
      "lyfte lyfted\n",
      "lying lying\n",
      "mabal maballs\n",
      "macwh macwhirter\n",
      "makem makemake\n",
      "malke malkethos\n",
      "manbe manbearpig\n",
      "mando mandolin\n",
      "maqui maquis\n",
      "marit marital\n",
      "marti martin\n",
      "match match\n",
      "maxke maxkeyboard\n",
      "mccol mccollum\n",
      "mcdoo mcdoobie\n",
      "mcflo mcflopidiss\n",
      "mcgov mcgovern\n",
      "mcgre mcgregor\n",
      "mcsla mcslackens\n",
      "medat medatascientist\n",
      "meddi medding\n",
      "megag megagross\n",
      "membe members\n",
      "memph memphis\n",
      "menin meningitis\n",
      "menzo menzoberranzan\n",
      "merpa merpact\n",
      "mesam mesamus\n",
      "miche michelle\n",
      "midfi midfield\n",
      "milin milinkovic\n",
      "milli million\n",
      "minor minor\n",
      "misch mischief\n",
      "mistr mistress\n",
      "mitro mitrovic\n",
      "mmand mmand\n",
      "mmuni mmunity\n",
      "momed momed\n",
      "moocl mooclucking\n",
      "moral moral\n",
      "morav moravia\n",
      "morni morning\n",
      "mothe mother\n",
      "motre motre\n",
      "mrrob mrrobopuppy\n",
      "mubar mubarak\n",
      "munge munger\n",
      "munis munis\n",
      "murdo murdoch\n",
      "mydea mydearwatson\n",
      "myndi myndi\n",
      "naara naara\n",
      "nabey nabeyaki\n",
      "nanab nanaba\n",
      "nanam nanami\n",
      "narli narlix\n",
      "nasta nastasic\n",
      "nazij nazijew\n",
      "nbtex nbtexplorer\n",
      "nechr nechrochasm\n",
      "nedav nedavno\n",
      "neghb neghborhood\n",
      "nelly nelly\n",
      "neoli neoliberal\n",
      "nerek nerekhall\n",
      "newti newtime\n",
      "nicht nicht\n",
      "nikes nikes\n",
      "nimme nimmer\n",
      "nixxe nixxes\n",
      "nkand nkandla\n",
      "noese noesen\n",
      "nonde nondescript\n",
      "nothe nother\n",
      "notwa notwastingmyuber\n",
      "obama obama\n",
      "obama obama\n",
      "obama obama\n",
      "obama obama\n",
      "obama obama\n",
      "obama obama\n",
      "obamm obammy\n",
      "oberb oberbayern\n",
      "objav objavljuju\n",
      "obrun obruni\n",
      "obscu obscure\n",
      "obtuv obtuvo\n",
      "occup occupied\n",
      "occup occupied\n",
      "oddes oddest\n",
      "ogrun ogrun\n",
      "oneco onecoin\n",
      "onlye onlye\n",
      "opero operon\n",
      "opina opinar\n",
      "oroba orobably\n",
      "oshte oshtemo\n",
      "other other\n",
      "ovoni ovonics\n",
      "padhi padhi\n",
      "padsi padsicles\n",
      "paizo paizo\n",
      "pakis pakistan\n",
      "palas palas\n",
      "paniy paniyiri\n",
      "paral parallel\n",
      "paras parasite\n",
      "parti particular\n",
      "pease pease\n",
      "pecki pecking\n",
      "pemul pemulis\n",
      "penti pentium\n",
      "perec perect\n",
      "persh pershing\n",
      "perve pervert\n",
      "petrs petrs\n",
      "phatp phatphace\n",
      "phuck phuck\n",
      "pictu picture\n",
      "piece piece\n",
      "piece piece\n",
      "pintl pintle\n",
      "pisst pisstake\n",
      "plese plese\n",
      "plian pliant\n",
      "polic police\n",
      "polit political\n",
      "porno pornography\n",
      "potoa potoato\n",
      "pozn poznan\n",
      "prakt praktisch\n",
      "pratt pratt\n",
      "preci precisely\n",
      "predd predditors\n",
      "presi president\n",
      "presu presumably\n",
      "princ principle\n",
      "pripa pripara\n",
      "priro prirodno\n",
      "prise prise\n",
      "priso prison\n",
      "probu probuilds\n",
      "progl proglide\n",
      "propa propaganda\n",
      "psyhi psyhic\n",
      "publi public\n",
      "pulle pulled\n",
      "punis punishment\n",
      "purni purnima\n",
      "quell quell\n",
      "quote quote\n",
      "quote quote\n",
      "quote quote\n",
      "racco raccoon\n",
      "radra radradra\n",
      "raibo raibow\n",
      "rainb rainbow\n",
      "ranke ranked\n",
      "ranke ranked\n",
      "rapit rapite\n",
      "reala reala\n",
      "rebot rebottle\n",
      "recon reconsider\n",
      "reell reelly\n",
      "regis register\n",
      "repub republican\n",
      "repul repulsive\n",
      "rever reverse\n",
      "rhunt rhunter\n",
      "rican rican\n",
      "right right\n",
      "rigom rigomortis\n",
      "rimli rimlight\n",
      "ripho riphorn\n",
      "rober robert\n",
      "robin robin\n",
      "rockg rockgroin\n",
      "rocks rocks\n",
      "romov romove\n",
      "rosti rosti\n",
      "rosti rosti\n",
      "russe russell\n",
      "sabro sabroso\n",
      "sakki sakki\n",
      "santa santa\n",
      "sanus sanus\n",
      "sanyu sanyue\n",
      "satia satiated\n",
      "savor savory\n",
      "sayth saything\n",
      "scarb scarborough\n",
      "schea scheana\n",
      "schme schmeichel\n",
      "schmu schmuck\n",
      "schoo school\n",
      "schoo school\n",
      "scoun scoundrel\n",
      "scrib scribe\n",
      "scrot scrotum\n",
      "sebel sebelius\n",
      "seduz seduzione\n",
      "sefer sefer\n",
      "sense sense\n",
      "sexyt sexytimes\n",
      "shamm shammy\n",
      "share share\n",
      "shear shear\n",
      "shenj shenjianbao\n",
      "shick shick\n",
      "shola shola\n",
      "shult shultz\n",
      "shult shultz\n",
      "siln silnylon\n",
      "simbo simbol\n",
      "singe singer\n",
      "siras siras\n",
      "siras siras\n",
      "sitm sitmonchai\n",
      "sketc sketchy\n",
      "skipi skiping\n",
      "slamt slamthedog\n",
      "sludg sludge\n",
      "slutc slutcake\n",
      "smear smear\n",
      "smell smell\n",
      "smith smith\n",
      "smoki smoking\n",
      "socal socal\n",
      "somic somic\n",
      "soona soona\n",
      "sotto sotto\n",
      "sowan sowan\n",
      "spals spalsh\n",
      "speci specific\n",
      "speec speech\n",
      "speku spekulation\n",
      "spide spider\n",
      "spiel spielberg\n",
      "spolu spolumbo\n",
      "sprin spring\n",
      "spurs spurs\n",
      "squee squeeze\n",
      "stacc staccato\n",
      "stebb stebbins\n",
      "steue steuer\n",
      "steve steve\n",
      "sthrn sthrn\n",
      "stinc stinchcomb\n",
      "storo storo\n",
      "story story\n",
      "stuck stuck\n",
      "stuck stuck\n",
      "studi studies\n",
      "stupi stupid\n",
      "stupi stupid\n",
      "stupi stupid\n",
      "stupi stupid\n",
      "stush stushoe\n",
      "sunme sunmer\n",
      "swuis swuish\n",
      "sydne sydney\n",
      "synne synnergy\n",
      "tambu tamburello\n",
      "tamra tamra\n",
      "tanat tanatie\n",
      "tatto tattoo\n",
      "tatto tattoo\n",
      "teach teacher\n",
      "teapi teapigs\n",
      "teled teledildonics\n",
      "telef telefon\n",
      "tengo tengo\n",
      "tenky tenkyou\n",
      "tenlo tenlock\n",
      "thadi thadius\n",
      "thatd thatd\n",
      "thefa thefappening\n",
      "thego thegorean\n",
      "there there\n",
      "think think\n",
      "thoma thomas\n",
      "thret thretened\n",
      "thurl thurlow\n",
      "tidel tideland\n",
      "tortu torture\n",
      "toshi toshiba\n",
      "track track\n",
      "trade trade\n",
      "tradu traduction\n",
      "trata trata\n",
      "troll troll\n",
      "troll troll\n",
      "trolo trololol\n",
      "truem trueman\n",
      "truth truth\n",
      "truth truth\n",
      "twitt twitter\n",
      "tyroi tyroil\n",
      "uglya uglyass\n",
      "umhum umhum\n",
      "uncoo uncool\n",
      "unden undeniable\n",
      "unkno unknown\n",
      "unort unorthodox\n",
      "unper unperturbed\n",
      "unre unrelated\n",
      "usbek usbekistan\n",
      "usinf usinf\n",
      "ustaa ustaad\n",
      "utopi utopia\n",
      "vacma vacmaster\n",
      "varza varza\n",
      "verda verdandi\n",
      "vergi vergil\n",
      "versc verschil\n",
      "verst verstappen\n",
      "vetti vetting\n",
      "viced viced\n",
      "vilon vilonia\n",
      "vilse vilseck\n",
      "viola violation\n",
      "vitup vituperative\n",
      "vlast vlast\n",
      "vollt vollta\n",
      "wackk wackk\n",
      "waggo waggoner\n",
      "warra warranty\n",
      "wayni wayning\n",
      "weare wearer\n",
      "whati whatisthisthing\n",
      "whini whining\n",
      "whitm whitman\n",
      "whooz whoozy\n",
      "whorl whorl\n",
      "willr willravel\n",
      "wilsh wilshere\n",
      "wimpl wimpled\n",
      "wired wired\n",
      "wuddl wuddly\n",
      "wwesu wwesupercard\n",
      "yanks yanks\n",
      "yarde yarder\n",
      "yorub yoruba\n",
      "yours yourself\n",
      "yours yourself\n",
      "zapre zapresic\n",
      "zinya zinyak\n",
      "zroth zroths\n",
      "zulus zulus\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "index = 5\n",
    "\n",
    "for word in missing_words:\n",
    "    \n",
    "    subs_word = word[:index]\n",
    "    \n",
    "    for model_word in model.index2word:\n",
    "        \n",
    "        if model_word.startswith(subs_word):\n",
    "            print(subs_word, model_word)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
