{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec model\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6580"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/clean_dataset.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique words in the dataset\n",
    "unique_words = []\n",
    "\n",
    "for comment in df['Comment']:\n",
    "    \n",
    "    for word in comment.split():\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF WORDS NOT IN MODEL:  1141\n",
      "PERCENTAGE MISSING:  5.916208648760759\n"
     ]
    }
   ],
   "source": [
    "# Check how many are not in the reddit model and collect them\n",
    "not_in_model = []\n",
    "\n",
    "for word in unique_words:\n",
    "    \n",
    "    if word not in model:\n",
    "        not_in_model.append(word)\n",
    "        \n",
    "print(\"NUMBER OF WORDS NOT IN MODEL: \", len(not_in_model))\n",
    "print(\"PERCENTAGE MISSING: \", (len(not_in_model)/len(unique_words)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the words then print them. Then, write them in a text file (DONE)\n",
    "#not_in_model = sorted(not_in_model)\n",
    "#with open('Missing words.txt', 'w') as fh:\n",
    "#    for word in not_in_model:\n",
    "#        fh.write(\"{}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146604\n"
     ]
    }
   ],
   "source": [
    "# Get the number of words in the Word2Vec model\n",
    "print(len(model.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arsehole', 0.8830243349075317),\n",
       " ('asshat', 0.8711205124855042),\n",
       " ('prick', 0.8389500975608826),\n",
       " ('dickhead', 0.8373932242393494),\n",
       " ('douchebag', 0.8241000175476074),\n",
       " ('dickbag', 0.8229827880859375),\n",
       " ('ahole', 0.8007912039756775),\n",
       " ('idiot', 0.7848369479179382),\n",
       " ('jackass', 0.7815600037574768),\n",
       " ('douche', 0.7751066088676453)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that are similar. This returns tuples in a list\n",
    "word = 'asshole'\n",
    "top_n = 10\n",
    "\n",
    "similar_words = model.most_similar(word, topn=top_n)\n",
    "model.most_similar(word, topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the Mean Cosine similarity among words\n",
    "import numpy as np\n",
    "\n",
    "mean_cos_distance = np.mean([ cos_distance for word, cos_distance in similar_words ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the collected words that are similar above this score\n",
    "words_above_mean = [word for word, cos_distance in similar_words if cos_distance > mean_cos_distance]\n",
    "total_words = float(len(words_above_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arsehole', 'asshat', 'prick', 'dickhead', 'douchebag', 'dickbag']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_above_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-initialize an empty numpy array (for speed)\n",
    "avgWordsFeature = np.zeros((300,),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over each word\n",
    "for word in words_above_mean:\n",
    "\n",
    "    # Add the word's vector\n",
    "    avgWordsFeature = np.add(avgWordsFeature,model[word])\n",
    "    \n",
    "# Average them out\n",
    "avgWordsFeature = np.divide(avgWordsFeature,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Use a numpy array to store each word as a feature\n",
    "# Use the first N-words. N is an integer\n",
    "\n",
    "sentenceWordFeature = np.zeros((5,), dtype=\"float32\")\n",
    "print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00470508  0.          0.          0.          0.        ]\n",
      "[-0.00470508  0.00158833  0.          0.          0.        ]\n",
      "[-0.00470508  0.00158833 -0.00082337  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Test with a single sentence\n",
    "\n",
    "sentence = \"you goddamn bastard\"\n",
    "complete_sentence = 1 # 1 for full, 0 for incomplete\n",
    "\n",
    "for i,word in enumerate(sentence.split()):\n",
    "    \n",
    "    if i == len(sentenceWordFeature):\n",
    "        complete_sentence = 0\n",
    "        break\n",
    "        \n",
    "    if word in model:\n",
    "        word_feature = np.mean(model[word])\n",
    "    else:\n",
    "        word_feature = -1.0\n",
    "        \n",
    "    sentenceWordFeature[i] = word_feature\n",
    "    \n",
    "    print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Soundex Dictionary (Normalized)\n",
    "soundex_dictionary = { \n",
    "    'B': \"1\", \"F\": \"1\", \"P\": \"1\",\n",
    "    \"V\": \"1\", \"C\": \"2\", \"G\": \"2\",\n",
    "    \"J\": \"2\", \"K\": \"2\", \"Q\": \"2\",\n",
    "    \"S\": \"2\", \"X\": \"2\", \"Z\": \"2\", \n",
    "    \"D\": \"3\", \"T\": \"3\", \"L\": \"4\", \n",
    "    \"M\": \"5\", \"N\": \"5\", \"R\": \"6\", \n",
    "    \"A\": \".\", \"E\": \".\", \"I\": \".\",\n",
    "    \"O\": \".\", \"U\": \".\", \"Y\": \".\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def getSoundex(word):\n",
    "    \n",
    "    # Uppercase the word\n",
    "    word = word.upper()\n",
    "\n",
    "    # Get the first letter of the word\n",
    "    soundex = word[0]\n",
    "\n",
    "    # Skip the following letters\n",
    "    skip_dict = \"HW\"\n",
    "    word = [letter for letter in word[1:] if letter not in skip_dict]\n",
    "    word = \"\".join(word)\n",
    "\n",
    "    # Loop character by character (Start with 2nd character)\n",
    "    for char in word[0:]:\n",
    "\n",
    "        code = soundex_dictionary[char]\n",
    "\n",
    "        if code != soundex[-1]:\n",
    "            soundex += code\n",
    "\n",
    "    # Replace period characters\n",
    "    soundex = soundex.replace(\".\", \"\")\n",
    "\n",
    "    # If the string has only one character, append rest with three 0s.\n",
    "    soundex = soundex[:4].ljust(4, \"0\")\n",
    "\n",
    "    return soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F230'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSoundex('fucked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Soundex dictionary\n",
    "import pickle\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/soundex_dict.pk\"\n",
    "\n",
    "soundex_dict = pickle.load(open(FILE,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL WORDS: 1146604\n",
      "UNIQUE SOUNDEX VALUES: 6599\n",
      "PERCENTAGE:  0.5755256391919092\n"
     ]
    }
   ],
   "source": [
    "# Perform operations here\n",
    "print(\"TOTAL WORDS: %i\" % (len(soundex_dict)))\n",
    "\n",
    "unique_soundex = list(set([value for key, value in soundex_dict.items()]))\n",
    "\n",
    "print(\"UNIQUE SOUNDEX VALUES: %i\" % (len(unique_soundex)))\n",
    "\n",
    "print(\"PERCENTAGE: \", (len(unique_soundex)/len(soundex_dict)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in missing word text file\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Text Files/Missing words.txt\"\n",
    "missing_words = []\n",
    "\n",
    "with open(FILE, 'r') as fh:\n",
    "    \n",
    "    # Iterate line by line\n",
    "    # Remove new line characters\n",
    "    for line in fh:\n",
    "        missing_words.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILAR WORDS FOUND: 1141\n"
     ]
    }
   ],
   "source": [
    "# Count how many missing words are in the soundex category\n",
    "count = 0\n",
    "\n",
    "for word in missing_words:\n",
    "    \n",
    "    soundex_encode = getSoundex(word)\n",
    "    \n",
    "    if soundex_encode in unique_soundex:\n",
    "        count += 1\n",
    "        \n",
    "print(\"SIMILAR WORDS FOUND: %i\" % (count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n"
     ]
    }
   ],
   "source": [
    "# Load the transformed file\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/soundex_words_list.pk\"\n",
    "\n",
    "soundex_words_list = pickle.load(open(FILE,\"rb\"))\n",
    "\n",
    "soundex = getSoundex('fuckingloser')\n",
    "word_list = soundex_words_list[soundex]\n",
    "\n",
    "print(len(word_list))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
