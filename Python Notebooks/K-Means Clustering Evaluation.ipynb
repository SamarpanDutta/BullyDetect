{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Evaluation\n",
    "\n",
    "This Python notebook is used for evaluation of a dictionary that is produced by:\n",
    "\n",
    "- Find the cluster a word belongs to \n",
    "- Find the other words in the cluster for a specific word\n",
    "- Compare between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL WORDS: 1146604 \n",
      "\n",
      "AVERAGE PER CLUSTER (250): 4586\n",
      "AVERAGE PER CLUSTER (500): 2293\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the files\n",
    "FILE_DICT_250 = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/dict_250C.pk\"\n",
    "FILE_CLUS_250 = \"C:/Users/MyPC/Desktop/Vegito/K-Means Models/full_250C.pk\"\n",
    "\n",
    "FILE_DICT_500 = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/dict_500C.pk\"\n",
    "FILE_CLUS_500 = \"C:/Users/MyPC/Desktop/Vegito/K-Means Models/full_500C.pk\"\n",
    "\n",
    "# Load using pickle\n",
    "array_dict_cluster_250 = pickle.load(open(FILE_DICT_250, \"rb\"))\n",
    "word_centroid_map_250 =  pickle.load(open(FILE_CLUS_250,\"rb\"))\n",
    "\n",
    "array_dict_cluster_500 = pickle.load(open(FILE_DICT_500, \"rb\"))\n",
    "word_centroid_map_500 =  pickle.load(open(FILE_CLUS_500,\"rb\"))\n",
    "\n",
    "total_clusters_250 = max(word_centroid_map_250.values()) + 1\n",
    "total_clusters_500 = max(word_centroid_map_500.values()) + 1\n",
    "\n",
    "average_word_250 = round(len(word_centroid_map_250)/total_clusters_250)\n",
    "average_word_500 = round(len(word_centroid_map_500)/total_clusters_500)\n",
    "\n",
    "# Display results\n",
    "print(\"TOTAL WORDS: %i \\n\" % (len(word_centroid_map_250)))\n",
    "\n",
    "print(\"AVERAGE PER CLUSTER (250): %i\" % (average_word_250))\n",
    "print(\"AVERAGE PER CLUSTER (500): %i\" % (average_word_500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCHED WORD: scumbag \n",
      "\n",
      "TOTAL WORDS (250): 3385\n",
      "TOTAL WORDS (500): 1148 \n",
      "\n",
      "WORDS (250):  ['creepster', 'spasticated', 'poseur', 'barrista', 'lyncher', 'assbag', 'douce', 'asahole', 'bitchboy', 'plebeian', 'indulger', 'nutzo', 'schooler', 'sterotype', 'turbonerd', 'showbusiness', 'astroturfer', 'hosebeast', 'fop', 'traitorous', 'trifflin', 'nooblet', 'shithawk', 'shmoe', 'mongoloid', 'interneter', 'ungratefull', 'joykill', 'edumacated', 'toity', 'struggler', 'ludite', 'coldblooded', 'crossfiter', 'scheister', 'softhearted', 'insufferable', 'salty', 'bandwagonner', 'nigged', 'comie', 'plebian', 'bogan', 'mastermind', 'whingey', 'punker', 'scumhole', 'pigkin', 'slaphead', 'schlubby', 'peson', 'bollocking', 'runescaper', 'dindunuffin', 'lapdog', 'fuckpipe', 'meatbag', 'smarmy', 'douchie', 'whore', 'trekkie', 'cockface', 'milquetoast', 'cuntself', 'kneckbeard', 'fag', 'rapie', 'bratty', 'schitck', 'bumptious', 'telemarketer', 'tarded', 'cornball', 'hardon', 'mewling', 'buttheaded', 'scumlord', 'jibbering', 'wierds', 'shrimpdittle', 'champing', 'torpid', 'fuckee', 'fogie', 'narky', 'scumbug', 'derro', 'disgrace', 'babykiller', 'merican', 'goony', 'raconteur', 'goofus', 'douchbag', 'dadbie', 'atics', 'shitbreath', 'assburgers', 'philantropist', 'assmonkey', 'cuckboy', 'skateboarder', 'nitwit', 'germophobe', 'shitfart', 'douchefag', 'gringo', 'snob', 'waahmbulance', 'wierdo', 'chickenhawk', 'toitey', 'cuntflaps', 'sketchball', 'seconder', 'aficionado', 'turdblossom', 'joning', 'jackanapes', 'agitator', 'trekie', 'durianrider', 'spaz', 'techie', 'dickface', 'eurotrash', 'filmer', 'jackwad', 'sissified', 'fearection', 'shtick', 'trendsetter', 'cuntastic', 'meatheaded', 'fatlogician', 'skitzo', 'junkie', 'thuggish', 'dumass', 'filfthy', 'goober', 'poncy', 'mullato', 'quitter', 'bazillionaire', 'sissy', 'cuddler', 'smackhead', 'buzzkill', 'lackie', 'cringelord', 'shitter', 'teenager', 'surfie', 'buttfuck', 'dickwagon', 'memeber', 'doully', 'nympho', 'smacktalking', 'chinless', 'blamer', 'doucehbag', 'speeks', 'looker', 'keystar', 'sconnie', 'gormless', 'cousinfucker', 'idgit', 'cusser', 'hoper', 'clouted', 'rapscallion', 'upitty', 'channer', 'bully', 'waffler', 'sanctimommy', 'twunt', 'pleb', 'illuminutty', 'ohioan', 'dogfucker', 'grunger', 'conniption', 'ameritard', 'uniballer', 'millionare', 'poopoohead', 'stepchild', 'cinephile', 'creepazoid', 'assholr', 'bossy', 'charlottean', 'shitrooster', 'stupis', 'underfucked', 'murkan'] \n",
      "\n",
      "\n",
      "WORDS (500):  ['mong', 'suckup', 'elitest', 'grubber', 'weebo', 'moralfag', 'sucker', 'raver', 'tattle', 'punkass', 'autistic', 'manlet', 'drunkard', 'cockhead', 'pyschopath', 'whitebread', 'coattail', 'arsewipe', 'downer', 'conniption', 'cholo', 'wristed', 'whinger', 'twatbag', 'douce', 'limey', 'pervert', 'retardo', 'coalburner', 'gger', 'windbag', 'genwunner', 'gormless', 'junkie', 'pendantic', 'slanderer', 'stranger', 'nosed', 'snit', 'beaner', 'sellout', 'cuntnugget', 'nutbag', 'spergy', 'wastrel', 'swayer', 'dickass', 'beatnik', 'whinny', 'opportunist', 'botherer', 'weab', 'sleazeball', 'kindergartener', 'pederast', 'pommy', 'hyprocrite', 'simpering', 'lier', 'smarmy', 'crackwhore', 'obeast', 'nebbish', 'douchbag', 'deadbeat', 'sleeze', 'prick', 'turdbucket', 'bookworm', 'madman', 'superfan', 'sonofabitch', 'wackjob', 'pussywhipped', 'hypocrite', 'pisser', 'necrophile', 'dullard', 'dumbfuck', 'scum', 'plebeian', 'niggerfaggot', 'neckbearded', 'whore', 'jackwagon', 'landlubber', 'squeeker', 'skygod', 'lardbeast', 'goof', 'casul', 'bellend', 'fairweather', 'teenybopper', 'shitstirrer', 'tweeker', 'aloner', 'smartarse', 'sexpat', 'fantasist', 'lecher', 'dweeb', 'pretensious', 'sanctimommy', 'chode', 'methed', 'ignoramus', 'baiter', 'unpurple', 'tankie', 'metalhead', 'dingleberry', 'shitbird', 'bawler', 'ghoster', 'sheboon', 'mongoloid', 'extortionist', 'simp', 'hooah', 'alchy', 'bitchly', 'pissant', 'nublet', 'poser', 'dependa', 'kiddy', 'musclehead', 'jerkbag', 'culchie', 'shithole', 'manbaby', 'jigaboo', 'hellhole', 'floozy', 'dopehead', 'egotist', 'ladykiller', 'troller', 'skeeze', 'loudmouthed', 'dicksucker', 'snoozer', 'germophobe', 'druggie', 'tarded', 'weasel', 'brogrammer', 'sod', 'sweetheart', 'dickweed', 'shamer', 'philanderer', 'panderer', 'shitweasel', 'druglord', 'schooler', 'stickler', 'knacker', 'pedant', 'skank', 'dickbag', 'clopper', 'inbred', 'scalper', 'luddite', 'sonuvabitch', 'daywalker', 'cheesedick', 'gymrat', 'redditeur', 'luvvie', 'nigger', 'grubbing', 'scumball', 'normalfag', 'embellisher', 'ingrate', 'chucker', 'faggoty', 'pompous', 'jerkass', 'doucher', 'hippocrite', 'paedo', 'vigin', 'bully', 'braggart', 'wannabe', 'whackjob', 'baffoon', 'kisser', 'lacky', 'friendless', 'grouch', 'groupie', 'hamplanet', 'mooches', 'shitbrick', 'weaboo']\n"
     ]
    }
   ],
   "source": [
    "# Find the cluster of words, based on a given word\n",
    "search = \"scumbag\"\n",
    "\n",
    "# Get the key, or cluster number\n",
    "# NOTE: Different clusters can have same results\n",
    "cluster_num_250 = word_centroid_map_250[search]\n",
    "cluster_num_500 = word_centroid_map_500[search]\n",
    "\n",
    "# Return the array based on the cluster number\n",
    "words_250 = array_dict_cluster_250[cluster_num_250]['word_list']\n",
    "words_500 = array_dict_cluster_500[cluster_num_500]['word_list']\n",
    "\n",
    "# Display results\n",
    "print(\"SEARCHED WORD: %s \\n\" % (search))\n",
    "\n",
    "print(\"TOTAL WORDS (250): %i\" % (len(words_250)))\n",
    "print(\"TOTAL WORDS (500): %i \\n\" % (len(words_500)))\n",
    "\n",
    "print(\"WORDS (250): \", words_250[:200], \"\\n\\n\")\n",
    "print(\"WORDS (500): \", words_500[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Perform PCA on the word vectors before Clustering with K-Means\n",
    "# Reason: Too much computations crashed the workstation along with memory constrains\n",
    "# Idea: By reducing the 300-dimensions to N-dimension by PCA, perform clustering\n",
    "# NOTE: Find the total variation from N-dimensions\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Word2Vec model\n",
    "print(\"LOADING WORD2VEC MODEL \\n\\n\")\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING WORD VECTORS AND WORDS\n",
      "TRAINING PCA MODEL\n",
      "EXPLAINED VARIANCED RATIO:  0.893246290946\n",
      "TIME TAKEN:  18.7891743183136\n"
     ]
    }
   ],
   "source": [
    "WORDS = 100000\n",
    "\n",
    "# Get the word vectors and words\n",
    "print(\"GETTING WORD VECTORS AND WORDS\")\n",
    "word_vectors = model.syn0[:WORDS]\n",
    "words = model.index2word[:WORDS]\n",
    "\n",
    "# Initialize PCA model\n",
    "print(\"TRAINING PCA MODEL\")\n",
    "pca = PCA(n_components=200)\n",
    "\n",
    "start = time.time()\n",
    "pca_result = pca.fit_transform(word_vectors)\n",
    "end = time.time()\n",
    "\n",
    "# Get explained variance ratio\n",
    "explain_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "\n",
    "print('EXPLAINED VARIANCED RATIO: ', explain_ratio)\n",
    "print('TIME TAKEN: ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING K-MEANS WITH 250 CLUSTERS \n",
      "\n",
      "\n",
      "TIME TAKEN 7.588577747344971\n",
      "STORING IN DICTIONARY\n"
     ]
    }
   ],
   "source": [
    "# Perform Minibatch K-Means clustering\n",
    "# Use 250 Clusters\n",
    "CLUSTERS = 250\n",
    "k_means = MiniBatchKMeans(n_clusters = CLUSTERS)\n",
    "\n",
    "# Fit the model, get the centroid number and calculate time\n",
    "print(\"TRAINING K-MEANS WITH %i CLUSTERS \\n\\n\" % (CLUSTERS))\n",
    "start = time.time()\n",
    "idx = k_means.fit_predict(pca_result)\n",
    "end = time.time()\n",
    "\n",
    "print('TIME TAKEN', end-start)\n",
    "\n",
    "# Store it in a dictionary\n",
    "print('STORING IN DICTIONARY')\n",
    "word_centroid_map = dict(zip(words,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER NUMBER: 106\n",
      "NUMBER OF WORDS: 341 \n",
      "\n",
      "WORDS:  ['fijian', 'strayan', 'hungarian', 'tibetan', 'german', 'american', 'glaswegian', 'mizrahi', 'haida', 'sian', 'kazakh', 'yiddish', 'castilian', 'uighur', 'andalusian', 'occitan', 'heritages', 'slav', 'sweedish', 'cornish', 'pennsylvanian', 'texan', 'cascadian', 'virginian', 'belgian', 'afrikaans', 'mandarin', 'austrian', 'blackfoot', 'yugoslavian', 'argentinean', 'frisian', 'expatriate', 'gaelic', 'hmong', 'viennese', 'asain', 'islander', 'gurkha', 'venezuelan', 'latino', 'nigerian', 'ghanaian', 'nahuatl', 'xhosa', 'persian', 'congolese', 'afghani', 'zimbabwean', 'murican', 'norsk', 'slovakian', 'acadian', 'kannada', 'mongolian', 'eastern', 'salvadoran', 'greek', 'haitian', 'tagalog', 'finno', 'native', 'multilingual', 'bengali', 'sudanese', 'frenchman', 'algerian', 'arabian', 'kuwaiti', 'pashto', 'spaniard', 'kashmiri', 'paki', 'viet', 'westerner', 'african', 'subsaharan', 'brasilian', 'basques', 'multiracial', 'nationality', 'serbian', 'uruguayan', 'serb', 'oriental', 'egyptian', 'okinawan', 'scandanavian', 'descent', 'sephardi', 'zealander', 'austrailian', 'westernized', 'romanian', 'brummie', 'tajik', 'pashtun', 'cockney', 'jamaican', 'ugandan', 'armenian', 'malay', 'lingual', 'lakota', 'wetback', 'pakistani', 'vietnamese', 'rican', 'icelandic', 'portugese', 'desi', 'welshman', 'suomi', 'chinky', 'turkic', 'brittish', 'albanian', 'gringo', 'english', 'navajo', 'lankan', 'punjabi', 'filipina', 'tongan', 'hindustani', 'aryan', 'caucasoid', 'sichuan', 'maori', 'ethnically', 'macedonian', 'swahili', 'ontarian', 'utahn', 'uralic', 'asiatic', 'caucasian', 'belarusian', 'polynesian', 'regional', 'rastafarian', 'castillian', 'balinese', 'basque', 'moldovan', 'nicaraguan', 'croat', 'ethnic', 'cambodian', 'kenyan', 'peruvian', 'scottish', 'moroccan', 'bangladeshi', 'ukranian', 'somali', 'hokkien', 'malaysian', 'flemish', 'farsi', 'yooper', 'portuguese', 'afrikaner', 'welsh', 'irishman', 'dravidian', 'swiss', 'turkish', 'parisian', 'chinese', 'slovene', 'malayalam', 'sicilian', 'brazillian', 'celtic', 'eritrean', 'andean', 'asian', 'candian', 'trilingual', 'panamanian', 'mestizo', 'bulgarian', 'dutch', 'italian', 'lithuanian', 'haole', 'singaporean', 'australian', 'marathi', 'eurasian', 'bilingual', 'galician', 'european', 'foreign', 'quebecois', 'manchurian', 'catalan', 'azerbaijani', 'aborigine', 'californian', 'ethiopian', 'swedish', 'fluent', 'foriegn', 'philippine', 'quebecer', 'germanic', 'bosnian', 'taiwanese', 'bostonian', 'lowland', 'anglo', 'slovenian', 'negro', 'gypsie', 'cantonese', 'arabic', 'argentinian', 'scandinavian', 'hindi', 'choctaw', 'chineese', 'berber', 'finnish', 'merican', 'nepali', 'guangdong', 'slavic', 'ancestry', 'colombian', 'amerindian', 'gypsy', 'americanized', 'hopi', 'croatian', 'kaffir', 'negroid', 'cuban', 'uyghur', 'immigrated', 'indo', 'tunisian', 'brahmin', 'polish', 'ashkenazi', 'cypriot', 'argentine', 'aboriginal', 'englishman', 'sephardic', 'expat', 'spanish', 'ainu', 'somalian', 'telugu', 'georgian', 'honduran', 'nepalese', 'burmese', 'ecuadorian', 'mulatto', 'mississippian', 'belorussian', 'irish', 'latvian', 'foreigner', 'phoenician', 'ethnicity', 'walloon', 'easterner', 'inuit', 'mexican', 'salish', 'french', 'aussie', 'brit', 'icelander', 'altaic', 'naturalized', 'czech', 'guatemalan', 'russian', 'british', 'romani', 'estonian', 'columbian', 'hispanic', 'lebanese', 'uzbek', 'westernised', 'norwegian', 'anatolian', 'hakka', 'indian', 'pakeha', 'fujian', 'melayu', 'subcontinent', 'emirati', 'anglophone', 'nordic', 'laotian', 'hawaiian', 'japenese', 'indonesian', 'chicano', 'slovak', 'bolivian', 'ugric', 'urdu', 'carib', 'javanese', 'gujarati', 'heritage', 'turk', 'korean', 'immigrant', 'metis', 'mainlander', 'briton', 'filipino', 'canadian', 'londoner', 'indigenous', 'norweigan', 'scotian', 'francophone', 'faroese', 'chilean', 'bantu', 'tamil', 'japanese', 'carolinian', 'danish', 'creole', 'liberian', 'swede', 'brazilian', 'monolingual', 'multiethnic']\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "word = 'australian'\n",
    "\n",
    "# Get cluster number\n",
    "cluster = word_centroid_map[word]\n",
    "\n",
    "# Append for words in same cluster\n",
    "word_list = [ word for word, cluster_num in word_centroid_map.items() if cluster == cluster_num ]\n",
    "\n",
    "print('CLUSTER NUMBER: %i' % (cluster))\n",
    "print('NUMBER OF WORDS: %i \\n' % (len(word_list)))\n",
    "print('WORDS: ' ,word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING K-MEANS WITH 250 CLUSTERS\n",
      "TIME TAKEN 364.2997696399689\n",
      "STORING IN DICTIONARY\n"
     ]
    }
   ],
   "source": [
    "# Perform K-Means clustering\n",
    "# Use 250 Clusters\n",
    "CLUSTERS = 250\n",
    "\n",
    "print('TRAINING K-MEANS WITH %i CLUSTERS' % (CLUSTERS))\n",
    "k_means = KMeans( n_clusters = CLUSTERS, n_jobs=6, precompute_distances=True)\n",
    "\n",
    "start = time.time()\n",
    "idx = k_means.fit_predict(pca_result)\n",
    "end = time.time()\n",
    "\n",
    "print('TIME TAKEN', end-start)\n",
    "# Store it in a dictionary\n",
    "print('STORING IN DICTIONARY')\n",
    "word_centroid_map = dict(zip(words,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF WORDS: 306 \n",
      "\n",
      "WORDS:  ['ultramarine', 'fading', 'lustrous', 'greyish', 'neon', 'faded', 'pale', 'turquoise', 'white', 'colorblind', 'discoloration', 'translucent', 'shimmering', 'pallid', 'illuminated', 'vibrant', 'yellowish', 'bluey', 'yellowed', 'yellowy', 'dimmed', 'aquamarine', 'backround', 'bluish', 'colors', 'colorings', 'diffused', 'playside', 'recolored', 'fringing', 'tinged', 'heterochromia', 'ornate', 'stripe', 'light', 'metalic', 'colours', 'soft', 'tinting', 'overexposed', 'browny', 'glittery', 'mismatching', 'colored', 'rosy', 'goldish', 'silky', 'blacklight', 'blurr', 'color', 'lowlights', 'skin', 'crisp', 'oversaturated', 'blurry', 'yellows', 'featureless', 'accentuating', 'red', 'illumination', 'glinting', 'shiney', 'contrasts', 'coppery', 'smudge', 'ashy', 'blocky', 'irises', 'nuln', 'tans', 'lilac', 'streaked', 'obscured', 'magenta', 'blueish', 'blue', 'underexposed', 'brightened', 'sparkled', 'purpley', 'swirly', 'brownish', 'yellow', 'gelled', 'sunlit', 'drab', 'plasticky', 'florescent', 'glassy', 'darkened', 'airbrushed', 'sallow', 'reddish', 'matchy', 'smudgy', 'metallic', 'grey', 'illuminating', 'gaudy', 'brassy', 'swirls', 'glisten', 'glowy', 'whitened', 'colour', 'darker', 'garish', 'indistinct', 'background', 'blotch', 'orangish', 'pearly', 'undertone', 'ruddy', 'marbled', 'fainter', 'greeny', 'adorning', 'transparent', 'distinctive', 'contrast', 'dusky', 'dark', 'specks', 'plumage', 'hues', 'blueness', 'swoosh', 'silhouettes', 'grays', 'darkening', 'lightened', 'monochrome', 'painted', 'camouflage', 'flecked', 'multicoloured', 'dye', 'checkerboard', 'fades', 'speckles', 'foreground', 'green', 'gray', 'pinkish', 'blotches', 'contrasty', 'fuzzy', 'visible', 'blending', 'filigree', 'purplish', 'rosette', 'opaque', 'blurred', 'paintjob', 'wavy', 'blurs', 'blobby', 'recoloured', 'toned', 'brights', 'highlights', 'gleamed', 'halos', 'paleness', 'yellowing', 'undertones', 'crease', 'lightish', 'fluorescent', 'blemishes', 'shimmers', 'desaturate', 'markings', 'splotches', 'violets', 'shade', 'silvery', 'wispy', 'pearlescent', 'orangey', 'beige', 'desaturated', 'brightly', 'fade', 'brighten', 'mottled', 'orange', 'tan', 'rosey', 'smudges', 'brown', 'sparkle', 'agrax', 'glows', 'creasing', 'stripes', 'dyed', 'creased', 'unevenness', 'velvety', 'accentuate', 'checkered', 'whited', 'tinted', 'flourescent', 'patterning', 'thinned', 'pastel', 'fluttery', 'complexions', 'sublimated', 'bright', 'shading', 'camouflaged', 'blotchy', 'flickering', 'hue', 'spotlights', 'tint', 'speckled', 'lighted', 'accenting', 'sparkles', 'hazy', 'saturated', 'tricolor', 'coloured', 'silhouette', 'blackish', 'incandescent', 'splotchy', 'redder', 'creases', 'patchy', 'shine', 'glowing', 'bluer', 'shadowed', 'coloration', 'cyan', 'inky', 'weathered', 'redish', 'discolouration', 'mismatched', 'darkish', 'grainy', 'shades', 'luminescent', 'purple', 'colouration', 'greenish', 'sheens', 'bleached', 'multicolored', 'splotch', 'tinge', 'monochromatic', 'whitish', 'sclera', 'smudged', 'pastels', 'accentuated', 'silhouetted', 'paler', 'chalky', 'teal', 'bronzed', 'darken', 'discoloured', 'glow', 'adorned', 'glare', 'springy', 'drybrush', 'sleek', 'pink', 'muted', 'darkens', 'glossy', 'squint', 'yellower', 'textured', 'sparkly', 'rippled', 'glowed', 'black', 'brighter', 'darks', 'feathery', 'obscuring', 'iridescent', 'grayish', 'wrinkles', 'blotted', 'colourblind', 'flecks', 'contrasting', 'shaded', 'reflective', 'smooth', 'backdrop', 'sepia', 'uncolored']\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "word = 'yellow'\n",
    "\n",
    "# Get cluster number\n",
    "cluster = word_centroid_map[word]\n",
    "\n",
    "# Append for words in same cluster\n",
    "word_list = [ word for word, cluster_num in word_centroid_map.items() if cluster == cluster_num ]\n",
    "        \n",
    "print('NUMBER OF WORDS: %i \\n' % (len(word_list)))        \n",
    "print('WORDS: ' ,word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
