{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Unigram Testing\n",
    "\n",
    "This Python Notebook is used for evaluation of the Word2Vec Unigram model. The section is broken down as follows:\n",
    "\n",
    "- Find most similar words from the selected word\n",
    "- Perform Syntactic Analysis\n",
    "- Perform Semantic Analysis\n",
    "- Find uncommon word among a list of words\n",
    "- Find cosine similarity among two words\n",
    "- Find the frequency count of a word\n",
    "- Check if a word is in the model\n",
    "- Print preview a list of words\n",
    "- Visualisation of words in Vector Space using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Unigram model\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biopsychology', 0.740115225315094),\n",
       " ('astrochemistry', 0.7391058206558228),\n",
       " ('neuroendocrinologist', 0.7296165227890015),\n",
       " ('nanoscience', 0.7265405058860779),\n",
       " ('neuropharmacology', 0.7247588038444519),\n",
       " ('saltzberg', 0.7157706618309021),\n",
       " ('ethnomusicology', 0.7156946659088135),\n",
       " ('psychobiology', 0.7154250144958496),\n",
       " ('nueroscience', 0.7147186994552612),\n",
       " ('neuropsychiatry', 0.7140935659408569),\n",
       " ('ichthyology', 0.710540235042572),\n",
       " ('molbio', 0.7056220769882202),\n",
       " ('oenology', 0.7056138515472412),\n",
       " ('antropology', 0.7041956186294556),\n",
       " ('biopsych', 0.7037904858589172),\n",
       " ('neuroengineering', 0.7037561535835266),\n",
       " ('nanoengineering', 0.7024978995323181),\n",
       " ('psycholinguistics', 0.7002543210983276),\n",
       " ('bioanthropology', 0.6995773315429688),\n",
       " ('christmann', 0.698868989944458)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to find most similar words \n",
    "# One word for unigram: dragon, bleach, tottenham\n",
    "# Two words for bigram: dragon_ball, barack_obama (UNDERSCORE NEEDED + BIGRAM MODEL LOADED)\n",
    "model.most_similar(\"neuropsychopharmacology\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumpur', 0.6737101674079895),\n",
       " ('kuala', 0.6668090224266052),\n",
       " ('taipei', 0.6401477456092834),\n",
       " ('bangkok', 0.6113026142120361),\n",
       " ('penang', 0.5809809565544128),\n",
       " ('lampur', 0.5752942562103271),\n",
       " ('toyko', 0.5550657510757446),\n",
       " ('selangor', 0.5511509776115417),\n",
       " ('singapore', 0.5502724647521973),\n",
       " ('mumbai', 0.5481346249580383)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for semantic evaluation (Ex. King - man + woman is approximately equal to queen)\n",
    "model.most_similar(positive=[\"tokyo\",\"malaysia\"], negative=[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueish', 0.7298511266708374),\n",
       " ('greyish', 0.7232707738876343),\n",
       " ('bluish', 0.7149738669395447),\n",
       " ('pinkish', 0.705883264541626),\n",
       " ('purplish', 0.7028074264526367),\n",
       " ('brownish', 0.6946163773536682),\n",
       " ('grayish', 0.6922476887702942),\n",
       " ('reddish', 0.6911346316337585),\n",
       " ('yellowish', 0.6770833134651184),\n",
       " ('whitish', 0.6669460535049438)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for syntactic evaluation (Ex. walking - walk + swim is approximately equal to swimming)\n",
    "model.most_similar(positive=[\"greenish\",\"blue\"], negative=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check which word doesn't match among a group of words\n",
    "model.doesnt_match(\"blue green yellow apple\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24046405589195533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check similarity among two words\n",
    "model.similarity(\"titanic\",\"rose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times a specific word occured in the 2015 Dataset\n",
    "word = model.vocab['difu']\n",
    "type(word.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if word (Unigram) is in model. It is case-sensitive\n",
    "'Dragon' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 guncraft 872865\n",
      "1 majoran 99873\n",
      "2 rejailbreaking 826941\n",
      "3 nikakav 771899\n",
      "4 piperanci 710825\n",
      "5 cqxe 515776\n",
      "6 rsdtyler 507115\n",
      "7 sollie 435384\n",
      "8 boinx 455347\n",
      "9 clocked 1130326\n",
      "10 vikt 750859\n",
      "11 onresize 329468\n",
      "12 iwamiger 895423\n",
      "13 nickibee 326422\n",
      "14 polonais 314179\n",
      "15 iuq 956664\n",
      "16 atenolol 959437\n",
      "17 chooths 373156\n",
      "18 burrista 126450\n",
      "19 adustycraphopper 488767\n",
      "20 anothersoul 804725\n",
      "21 diavola 841148\n",
      "22 enounced 5970\n",
      "23 werburghs 646778\n",
      "24 biglands 569222\n",
      "25 spiderwheels 134090\n",
      "26 awda 154548\n",
      "27 croyder 384187\n",
      "28 akkarat 966142\n",
      "29 metrogel 860290\n",
      "30 ajchen 904863\n",
      "31 aethelric 849745\n",
      "32 belic 877375\n",
      "33 clickclacks 666515\n",
      "34 bikta 298037\n",
      "35 phillan 165512\n",
      "36 mizuki 1063805\n",
      "37 interfluidity 191334\n",
      "38 hydrodipping 870031\n",
      "39 inarajan 184989\n",
      "40 ketquaxosowap 475750\n",
      "41 impetuosity 582267\n",
      "42 zerth 433872\n",
      "43 komander 115511\n",
      "44 reharmonized 398025\n",
      "45 gillivray 930867\n",
      "46 bridgespan 337231\n",
      "47 advocators 860475\n",
      "48 sotrue 466741\n",
      "49 mathijssen 355133\n",
      "50 uins 269424\n",
      "51 mag 1140448\n",
      "52 heyo 1107494\n",
      "53 mechtoday 361952\n",
      "54 matrimonium 168405\n",
      "55 weaponize 1092723\n",
      "56 houdon 412408\n",
      "57 yoggscast 239488\n",
      "58 evosim 35701\n",
      "59 hqtn 18253\n",
      "60 budzet 592258\n",
      "61 matural 38330\n",
      "62 tocaria 385942\n",
      "63 fsch 68077\n",
      "64 khow 772488\n",
      "65 beautfil 360175\n",
      "66 geode 1075074\n",
      "67 shotokai 258471\n",
      "68 mamlucks 673655\n",
      "69 hanes 1103427\n",
      "70 forthenaughtythings 351993\n"
     ]
    }
   ],
   "source": [
    "# A brief review of words in the model\n",
    "count = 70\n",
    "\n",
    "for index, word in enumerate(model.vocab):\n",
    "    print(index, word, model.vocab[word].count)\n",
    "    if index == count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualisation (Normal) using TSNE\n",
    "# Motivation: http://lvdmaaten.github.io/tsne/\n",
    "# Video: https://www.youtube.com/watch?v=RJVL80Gg3lA\n",
    "\n",
    "# Firstly: Import the libraries\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create function to return list of words and word embeddings\n",
    "\n",
    "def getEmbeddings():\n",
    "    \n",
    "    # Arrays to store feature vector and words\n",
    "    vectors = []\n",
    "    words = []\n",
    "    \n",
    "    # Add vector and words\n",
    "    # Add vectors and words\n",
    "    for word in model.vocab:\n",
    "        \n",
    "        vectors.append(model[word]) \n",
    "        words.append(word)\n",
    "        \n",
    "    return vectors, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the graph in this cell (RANDOM WORDS)\n",
    "\n",
    "# Get the feature vectors and respective words\n",
    "wv, vocabulary = getEmbeddings()\n",
    "\n",
    "# Initialize TSNE model\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "\n",
    "# Fit with first 500 words \n",
    "Y = tsne.fit_transform(wv[10000:11000])\n",
    "\n",
    "# Scatter points\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Use Scatterplot\n",
    "ax.scatter(Y[:, 0], Y[:, 1], color='red')\n",
    "\n",
    "# Initialize Points\n",
    "for label, x, y in zip(vocabulary, Y[:, 0], Y[:, 1]):\n",
    "    ax.annotate(label, xy=(x, y), fontsize=15)\n",
    "\n",
    "# Display\n",
    "mpld3.display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
