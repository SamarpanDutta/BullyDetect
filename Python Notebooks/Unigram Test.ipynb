{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Unigram Testing\n",
    "\n",
    "This Python Notebook is used for evaluation of the Word2Vec Unigram model. The section is broken down as follows:\n",
    "\n",
    "- Find most similar words from the selected word\n",
    "- Perform Syntactic Analysis\n",
    "- Perform Semantic Analysis\n",
    "- Find uncommon word among a list of words\n",
    "- Find cosine similarity among two words\n",
    "- Find the frequency count of a word\n",
    "- Check if a word is in the model\n",
    "- Print preview a list of words\n",
    "- Others (Vector space size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Unigram model\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biopsychology', 0.740115225315094),\n",
       " ('astrochemistry', 0.7391058206558228),\n",
       " ('neuroendocrinologist', 0.7296165227890015),\n",
       " ('nanoscience', 0.7265405058860779),\n",
       " ('neuropharmacology', 0.7247588038444519),\n",
       " ('saltzberg', 0.7157706618309021),\n",
       " ('ethnomusicology', 0.7156946659088135),\n",
       " ('psychobiology', 0.7154250144958496),\n",
       " ('nueroscience', 0.7147186994552612),\n",
       " ('neuropsychiatry', 0.7140935659408569),\n",
       " ('ichthyology', 0.710540235042572),\n",
       " ('molbio', 0.7056220769882202),\n",
       " ('oenology', 0.7056138515472412),\n",
       " ('antropology', 0.7041956186294556),\n",
       " ('biopsych', 0.7037904858589172),\n",
       " ('neuroengineering', 0.7037561535835266),\n",
       " ('nanoengineering', 0.7024978995323181),\n",
       " ('psycholinguistics', 0.7002543210983276),\n",
       " ('bioanthropology', 0.6995773315429688),\n",
       " ('christmann', 0.698868989944458)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to find most similar words \n",
    "# One word for unigram: dragon, bleach, tottenham\n",
    "# Two words for bigram: dragon_ball, barack_obama (UNDERSCORE NEEDED + BIGRAM MODEL LOADED)\n",
    "model.most_similar(\"neuropsychopharmacology\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumpur', 0.6737101674079895),\n",
       " ('kuala', 0.6668090224266052),\n",
       " ('taipei', 0.6401477456092834),\n",
       " ('bangkok', 0.6113026142120361),\n",
       " ('penang', 0.5809809565544128),\n",
       " ('lampur', 0.5752942562103271),\n",
       " ('toyko', 0.5550657510757446),\n",
       " ('selangor', 0.5511509776115417),\n",
       " ('singapore', 0.5502724647521973),\n",
       " ('mumbai', 0.5481346249580383)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for semantic evaluation (Ex. King - man + woman is approximately equal to queen)\n",
    "model.most_similar(positive=[\"tokyo\",\"malaysia\"], negative=[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueish', 0.7298511266708374),\n",
       " ('greyish', 0.7232707738876343),\n",
       " ('bluish', 0.7149738669395447),\n",
       " ('pinkish', 0.705883264541626),\n",
       " ('purplish', 0.7028074264526367),\n",
       " ('brownish', 0.6946163773536682),\n",
       " ('grayish', 0.6922476887702942),\n",
       " ('reddish', 0.6911346316337585),\n",
       " ('yellowish', 0.6770833134651184),\n",
       " ('whitish', 0.6669460535049438)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for syntactic evaluation (Ex. walking - walk + swim is approximately equal to swimming)\n",
    "model.most_similar(positive=[\"greenish\",\"blue\"], negative=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check which word doesn't match among a group of words\n",
    "model.doesnt_match(\"blue green yellow apple\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24046405589195533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check similarity among two words\n",
    "model.similarity(\"titanic\",\"rose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times a specific word occured in the 2015 Dataset\n",
    "word = model.vocab['difu']\n",
    "type(word.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if word (Unigram) is in model. It is case-sensitive\n",
    "'Dragon' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 okkie 439508\n",
      "1 blows 1142077\n",
      "2 farokhmanesh 868069\n",
      "3 harqan 152515\n",
      "4 ecoupon 234460\n",
      "5 kmail 927823\n",
      "6 houtei 455986\n",
      "7 lse 1015078\n",
      "8 shedler 571945\n",
      "9 coworkes 572107\n",
      "10 lurpis 808069\n",
      "11 hhuuggee 501753\n",
      "12 perilaku 178095\n",
      "13 fritzbox 879686\n",
      "14 unhingedness 360368\n",
      "15 maritza 944746\n",
      "16 camelpacks 294028\n",
      "17 fancams 1044724\n",
      "18 sweatfests 240353\n",
      "19 consultation 1127028\n",
      "20 juzgados 907097\n",
      "21 noonmark 113457\n",
      "22 roasties 892514\n",
      "23 reyjavik 588103\n",
      "24 chattel 1105974\n",
      "25 badf 769341\n",
      "26 spyxe 894233\n",
      "27 newgrad 436663\n",
      "28 doneness 1085608\n",
      "29 perpetualperplex 99818\n",
      "30 crk 1060265\n",
      "31 saintliness 915133\n",
      "32 sorex 404134\n",
      "33 honzon 23022\n",
      "34 cruciamentum 741891\n",
      "35 metalsmithing 934140\n",
      "36 videogameobsession 242152\n",
      "37 walamart 164211\n",
      "38 bothies 336256\n",
      "39 volantese 359981\n",
      "40 celata 83881\n",
      "41 padden 817455\n",
      "42 unlikeable 1111388\n",
      "43 embarssing 819288\n",
      "44 metsler 150690\n",
      "45 diebacked 150467\n",
      "46 lokie 851823\n",
      "47 ames 1107869\n",
      "48 scotlanders 322660\n",
      "49 heseltine 843305\n",
      "50 asianing 137332\n",
      "51 werkweek 440430\n",
      "52 aldemedes 681600\n",
      "53 pulmonis 201372\n",
      "54 manisaspor 66014\n",
      "55 ahoy 1112713\n",
      "56 emenating 726731\n",
      "57 cheyna 443759\n",
      "58 ballesteros 770869\n",
      "59 livinpink 568010\n",
      "60 baloti 107753\n",
      "61 strops 1056843\n",
      "62 predicciones 593750\n",
      "63 environics 645567\n",
      "64 heiffers 16067\n",
      "65 miscalls 909337\n",
      "66 preorderd 1007848\n",
      "67 tallica 900023\n",
      "68 gouka 271672\n",
      "69 takasugi 989367\n",
      "70 hitbox 1133788\n"
     ]
    }
   ],
   "source": [
    "# A brief review of words in the model\n",
    "count = 70\n",
    "\n",
    "for index, word in enumerate(model.vocab):\n",
    "    print(index, word, model.vocab[word].count)\n",
    "    if index == count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking under the hood of Word2vec\n",
    "# Use this cell as tips for K-Means Clustering\n",
    "# Motivation: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors\n",
    "\n",
    "# Feature vector for each word(s) \n",
    "#print(model.syn0[0])\n",
    "\n",
    "# Shape of the vocabulary\n",
    "#print(model.syn0.shape)\n",
    "\n",
    "# Get list of all keys (words)\n",
    "#model.index2word[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME TAKEN:  2.908646821975708\n"
     ]
    }
   ],
   "source": [
    "# Do preliminary of K-Means testing using the first (1000,2000,4000) words using 250 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# Specify the number of words and clusters\n",
    "WORDS = 1000\n",
    "CLUSTERS = 250\n",
    "\n",
    "# Get the word vectors and the word\n",
    "word_vectors = model.syn0[:WORDS]\n",
    "words = model.index2word[:WORDS]\n",
    "\n",
    "# Initialize K-Means\n",
    "k_means = KMeans( n_clusters = CLUSTERS )\n",
    "\n",
    "# Fit the model, get the centroid number and calculate time\n",
    "start = time.time()\n",
    "idx = k_means.fit_predict(word_vectors)\n",
    "end = time.time()\n",
    "\n",
    "print(\"TIME TAKEN: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d5cf5580f604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mword_centroid_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_centroid_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object does not support indexing"
     ]
    }
   ],
   "source": [
    "# Create a Word / Index dictionary\n",
    "# Each vocabulary word is matched to a cluster center\n",
    "\n",
    "word_centroid_map = dict(zip( words, idx ))\n",
    "\n",
    "len(word_centroid_map.values()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Loop through the top N clusters\n",
    "\n",
    "N = 10\n",
    "\n",
    "for i in range(0, N):\n",
    "    \n",
    "    #Create array of words\n",
    "    words = []\n",
    "    \n",
    "    for i in range(0, WORDS):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
