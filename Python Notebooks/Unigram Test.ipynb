{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Unigram Testing\n",
    "\n",
    "This Python Notebook is used for evaluation of the Word2Vec Unigram model. The section is broken down as follows:\n",
    "\n",
    "- Find most similar words from the selected word\n",
    "- Perform Syntactic Analysis\n",
    "- Perform Semantic Analysis\n",
    "- Find uncommon word among a list of words\n",
    "- Find cosine similarity among two words\n",
    "- Find the frequency count of a word\n",
    "- Check if a word is in the model\n",
    "- Print preview a list of words\n",
    "- Others (Vector space size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Unigram model\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biopsychology', 0.740115225315094),\n",
       " ('astrochemistry', 0.7391058206558228),\n",
       " ('neuroendocrinologist', 0.7296165227890015),\n",
       " ('nanoscience', 0.7265405058860779),\n",
       " ('neuropharmacology', 0.7247588038444519),\n",
       " ('saltzberg', 0.7157706618309021),\n",
       " ('ethnomusicology', 0.7156946659088135),\n",
       " ('psychobiology', 0.7154250144958496),\n",
       " ('nueroscience', 0.7147186994552612),\n",
       " ('neuropsychiatry', 0.7140935659408569),\n",
       " ('ichthyology', 0.710540235042572),\n",
       " ('molbio', 0.7056220769882202),\n",
       " ('oenology', 0.7056138515472412),\n",
       " ('antropology', 0.7041956186294556),\n",
       " ('biopsych', 0.7037904858589172),\n",
       " ('neuroengineering', 0.7037561535835266),\n",
       " ('nanoengineering', 0.7024978995323181),\n",
       " ('psycholinguistics', 0.7002543210983276),\n",
       " ('bioanthropology', 0.6995773315429688),\n",
       " ('christmann', 0.698868989944458)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to find most similar words \n",
    "# One word for unigram: dragon, bleach, tottenham\n",
    "# Two words for bigram: dragon_ball, barack_obama (UNDERSCORE NEEDED + BIGRAM MODEL LOADED)\n",
    "model.most_similar(\"neuropsychopharmacology\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumpur', 0.6737101674079895),\n",
       " ('kuala', 0.6668090224266052),\n",
       " ('taipei', 0.6401477456092834),\n",
       " ('bangkok', 0.6113026142120361),\n",
       " ('penang', 0.5809809565544128),\n",
       " ('lampur', 0.5752942562103271),\n",
       " ('toyko', 0.5550657510757446),\n",
       " ('selangor', 0.5511509776115417),\n",
       " ('singapore', 0.5502724647521973),\n",
       " ('mumbai', 0.5481346249580383)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for semantic evaluation (Ex. King - man + woman is approximately equal to queen)\n",
    "model.most_similar(positive=[\"tokyo\",\"malaysia\"], negative=[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueish', 0.7298511266708374),\n",
       " ('greyish', 0.7232707738876343),\n",
       " ('bluish', 0.7149738669395447),\n",
       " ('pinkish', 0.705883264541626),\n",
       " ('purplish', 0.7028074264526367),\n",
       " ('brownish', 0.6946163773536682),\n",
       " ('grayish', 0.6922476887702942),\n",
       " ('reddish', 0.6911346316337585),\n",
       " ('yellowish', 0.6770833134651184),\n",
       " ('whitish', 0.6669460535049438)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for syntactic evaluation (Ex. walking - walk + swim is approximately equal to swimming)\n",
    "model.most_similar(positive=[\"greenish\",\"blue\"], negative=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check which word doesn't match among a group of words\n",
    "model.doesnt_match(\"blue green yellow apple\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24046405589195533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check similarity among two words\n",
    "model.similarity(\"titanic\",\"rose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times a specific word occured in the 2015 Dataset\n",
    "word = model.vocab['difu']\n",
    "type(word.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if word (Unigram) is in model. It is case-sensitive\n",
    "'Dragon' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cockweasel 708272\n",
      "1 frakken 449808\n",
      "2 smirfs 428731\n",
      "3 wiine 853778\n",
      "4 rellying 753562\n",
      "5 medsbuy 753745\n",
      "6 kirah 642904\n",
      "7 loltumblr 1027250\n",
      "8 confutation 86362\n",
      "9 desarmado 49277\n",
      "10 whicy 268221\n",
      "11 weekending 733123\n",
      "12 mactaggart 193721\n",
      "13 caprisun 1003736\n",
      "14 studierte 202535\n",
      "15 dourtmund 137290\n",
      "16 namors 626101\n",
      "17 gingered 700697\n",
      "18 urzak 346864\n",
      "19 alck 366608\n",
      "20 hronis 322693\n",
      "21 depraved 1120032\n",
      "22 perichondrium 157468\n",
      "23 tumakbo 799760\n",
      "24 ximum 550855\n",
      "25 bantam 1087697\n",
      "26 fullon 866146\n",
      "27 cachete 724355\n",
      "28 nakakaiyak 347117\n",
      "29 stoneberry 443653\n",
      "30 parallelisms 855916\n",
      "31 fueling 1122857\n",
      "32 eliminiation 405126\n",
      "33 budzilla 354658\n",
      "34 divestitures 729040\n",
      "35 badmannered 475022\n",
      "36 celticsblog 720527\n",
      "37 windtalkers 933967\n",
      "38 fururama 700825\n",
      "39 inevara 118029\n",
      "40 ertragswert 64683\n",
      "41 giffingtool 112915\n",
      "42 dingens 93462\n",
      "43 sommerdamm 674304\n",
      "44 dickade 55306\n",
      "45 conceration 23629\n",
      "46 ujarnya 188641\n",
      "47 iphy 359343\n",
      "48 receptiveness 1019798\n",
      "49 microcube 754417\n",
      "50 proudmanlet 159038\n",
      "51 sysinstall 78281\n",
      "52 cambada 491529\n",
      "53 bistrevsky 351495\n",
      "54 sablich 106899\n",
      "55 horngate 8558\n",
      "56 fekaf 91361\n",
      "57 partiel 346242\n",
      "58 tidebuy 656835\n",
      "59 blendon 438701\n",
      "60 hysterz 557559\n",
      "61 attenderen 265015\n",
      "62 sylverant 709755\n",
      "63 schicky 61459\n",
      "64 vaxine 173682\n",
      "65 zedxleppelin 230616\n",
      "66 solidblues 768659\n",
      "67 gensport 124034\n",
      "68 echobelly 668600\n",
      "69 jubilo 705610\n",
      "70 euler 1109608\n"
     ]
    }
   ],
   "source": [
    "# A brief review of words in the model\n",
    "count = 70\n",
    "\n",
    "for index, word in enumerate(model.vocab):\n",
    "    print(index, word, model.vocab[word].count)\n",
    "    if index == count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looking under the hood of Word2vec\n",
    "# Use this cell as tips for K-Means Clustering\n",
    "# Motivation: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors\n",
    "\n",
    "# Feature vector for each word(s) \n",
    "#print(model.syn0[0])\n",
    "\n",
    "# Shape of the vocabulary\n",
    "#print(model.syn0.shape)\n",
    "\n",
    "# Get list of all keys (words)\n",
    "#model.index2word[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME TAKEN:  3.9487807750701904\n"
     ]
    }
   ],
   "source": [
    "# Do preliminary of K-Means testing using the first (1000,2000,4000) words using 250 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "\n",
    "# Specify the number of words and clusters\n",
    "WORDS = 1000\n",
    "CLUSTERS = 250\n",
    "\n",
    "# Get the word vectors and the word\n",
    "word_vectors = model.syn0[:WORDS]\n",
    "words = model.index2word[:WORDS]\n",
    "\n",
    "# Initialize K-Means\n",
    "k_means = KMeans( n_clusters = CLUSTERS )\n",
    "\n",
    "# Fit the model, get the centroid number and calculate time\n",
    "start = time.time()\n",
    "idx = k_means.fit_predict(word_vectors)\n",
    "end = time.time()\n",
    "\n",
    "print(\"TIME TAKEN: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary\n",
    "# Each vocabulary word is matched to a cluster center\n",
    "\n",
    "word_centroid_map = dict(zip( words, idx ))\n",
    "\n",
    "#word_centroid_map.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER NUMBER: 0\n",
      "WORDS:  ['games', 'game', 'match', 'map']\n",
      "CLUSTER NUMBER: 1\n",
      "WORDS:  ['d', 'f', 'r', 'e', 'y', 'n', 'm', 'l', 'k', 'g', 'p', 'h', 's', 'v', 'o', 'c', 'j', 'z', 't', 'w', 'b']\n",
      "CLUSTER NUMBER: 2\n",
      "WORDS:  ['due', 'result', 'fix', 'pain', 'cause']\n",
      "CLUSTER NUMBER: 3\n",
      "WORDS:  ['fun', 'hot', 'nice', 'cool', 'interesting']\n",
      "CLUSTER NUMBER: 4\n",
      "WORDS:  ['walk', 'pass', 'run', 'running']\n",
      "CLUSTER NUMBER: 5\n",
      "WORDS:  ['thought', 'was', 'knew', 'were']\n",
      "CLUSTER NUMBER: 6\n",
      "WORDS:  ['clearly', 'basically', 'literally', 'obviously', 'apparently']\n",
      "CLUSTER NUMBER: 7\n",
      "WORDS:  ['of', 'otherwise', 'there', 'is', 'such', 'that', 'a', 'which', 'this', 'who', 'the', 'as', 'an', 'right']\n",
      "CLUSTER NUMBER: 8\n",
      "WORDS:  ['hold', 'stand', 'pull']\n",
      "CLUSTER NUMBER: 9\n",
      "WORDS:  ['choice', 'choose', 'pick']\n",
      "CLUSTER NUMBER: 10\n",
      "WORDS:  ['woman', 'she', 'women', 'person', 'girl', 'girls', 'men']\n",
      "CLUSTER NUMBER: 11\n",
      "WORDS:  ['value', 'cost', 'price', 'worth']\n",
      "CLUSTER NUMBER: 12\n",
      "WORDS:  ['likely', 'common', 'popular']\n",
      "CLUSTER NUMBER: 13\n",
      "WORDS:  ['watch', 'watching', 'watched']\n",
      "CLUSTER NUMBER: 14\n",
      "WORDS:  ['size', 'small', 'large']\n",
      "CLUSTER NUMBER: 15\n",
      "WORDS:  ['telling', 'told', 'ask', 'trust', 'tell', 'explain']\n",
      "CLUSTER NUMBER: 16\n",
      "WORDS:  ['couple', 'two', 'few', 'three', 'both', 'one', 'multiple', 'together', 'several']\n",
      "CLUSTER NUMBER: 17\n",
      "WORDS:  ['class', 'college', 'school']\n",
      "CLUSTER NUMBER: 18\n",
      "WORDS:  ['kinda', 'kind', 'sort']\n",
      "CLUSTER NUMBER: 19\n",
      "WORDS:  ['information', 'data', 'advice', 'info']\n",
      "CLUSTER NUMBER: 20\n",
      "WORDS:  ['guy', 'guys', 'dick', 'dude', 'man', 'fucking']\n",
      "CLUSTER NUMBER: 21\n",
      "WORDS:  ['bot', 'pu', 'performed', 'automatically', 'action', 'automoderator']\n",
      "CLUSTER NUMBER: 22\n",
      "WORDS:  ['me', 'us', 'him', 'them', 'his', 'himself', 'he', 'her']\n",
      "CLUSTER NUMBER: 23\n",
      "WORDS:  ['pay', 'attention', 'paying', 'paid', 'money']\n",
      "CLUSTER NUMBER: 24\n",
      "WORDS:  ['say', 'fact', 'saying', 'calling', 'asking']\n",
      "CLUSTER NUMBER: 25\n",
      "WORDS:  ['concerns', 'trade', 'question', 'questions']\n",
      "CLUSTER NUMBER: 26\n",
      "WORDS:  ['well', 'though', 'so', 'except', 'especially', 'but', 'however', 'and', 'although', 'plus']\n",
      "CLUSTER NUMBER: 27\n",
      "WORDS:  ['feel', 'feels', 'feeling', 'felt']\n",
      "CLUSTER NUMBER: 28\n",
      "WORDS:  ['white', 'black']\n",
      "CLUSTER NUMBER: 29\n",
      "WORDS:  ['country', 'states', 'state']\n",
      "CLUSTER NUMBER: 30\n",
      "WORDS:  ['reddit', 'subreddit', 'sub']\n",
      "CLUSTER NUMBER: 31\n",
      "WORDS:  ['difficult', 'easy', 'important', 'hard', 'easier']\n",
      "CLUSTER NUMBER: 32\n",
      "WORDS:  ['next', 'last', 'third', 'th', 'second', 'only', 'first']\n",
      "CLUSTER NUMBER: 33\n",
      "WORDS:  ['personally', 'honestly', 'seriously', 'i', 'honest', 'even', 'really', 'actually', 'about']\n",
      "CLUSTER NUMBER: 34\n",
      "WORDS:  ['its', 'itself', 'mine', 'it']\n",
      "CLUSTER NUMBER: 35\n",
      "WORDS:  ['year']\n",
      "CLUSTER NUMBER: 36\n",
      "WORDS:  ['main', 'side', 'note', 'hand']\n",
      "CLUSTER NUMBER: 37\n",
      "WORDS:  ['hands', 'face', 'head', 'eye', 'eyes', 'heart']\n",
      "CLUSTER NUMBER: 38\n",
      "WORDS:  ['some', 'lot', 'plenty', 'lots', 'none', 'bunch', 'many', 'amount']\n",
      "CLUSTER NUMBER: 39\n",
      "WORDS:  ['fall', 'drop', 'double']\n",
      "CLUSTER NUMBER: 40\n",
      "WORDS:  ['higher', 'level', 'above', 'low', 'high', 'lower', 'rate']\n",
      "CLUSTER NUMBER: 41\n",
      "WORDS:  ['friends', 'family']\n",
      "CLUSTER NUMBER: 42\n",
      "WORDS:  ['server', 'site', 'page', 'com']\n",
      "CLUSTER NUMBER: 43\n",
      "WORDS:  ['hope', 'soon', 'finally', 'hopefully', 'glad']\n",
      "CLUSTER NUMBER: 44\n",
      "WORDS:  ['given', 'giving', 'give', 'gave', 'gives']\n",
      "CLUSTER NUMBER: 45\n",
      "WORDS:  ['majority', 'rest']\n",
      "CLUSTER NUMBER: 46\n",
      "WORDS:  ['quick', 'speed', 'fast', 'slow', 'quickly']\n",
      "CLUSTER NUMBER: 47\n",
      "WORDS:  ['bought', 'sell', 'buying', 'buy']\n",
      "CLUSTER NUMBER: 48\n",
      "WORDS:  ['issue', 'problems', 'issues', 'problem']\n",
      "CLUSTER NUMBER: 49\n",
      "WORDS:  ['okay', 'ok', 'hey', 'fine', 'fair']\n"
     ]
    }
   ],
   "source": [
    "# Loop through the top N clusters\n",
    "\n",
    "N = 50\n",
    "\n",
    "for cluster in range(0, N):\n",
    "    \n",
    "    #Create array of words\n",
    "    words = []\n",
    "    \n",
    "    for word, cluster_num in word_centroid_map.items():\n",
    "        if cluster_num == cluster:\n",
    "            words.append(word)\n",
    "    \n",
    "    print(\"CLUSTER NUMBER: %i\" % (cluster))\n",
    "    print(\"WORDS: \", words)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
