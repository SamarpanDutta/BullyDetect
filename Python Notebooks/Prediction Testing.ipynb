{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Testing\n",
    "\n",
    "Randomly testing comments. Currently using Dr. Soon Lay Ki's method of mean similarity:\n",
    "\n",
    "- For a specific word, find the top 5 similar words\n",
    "- Find the cosine mean of those words\n",
    "- Find words that are above that mean\n",
    "- Sum up the feature vectors per word and average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries here\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load necessary models\n",
    "\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dr. Soon's idea (Use Top 5 words)\n",
    "def wordsAverage(word):\n",
    "\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    # 300 is used, as it is the number of vectors in Word2Vec\n",
    "    avgWordsFeature = np.zeros((300,),dtype=\"float32\")\n",
    "\n",
    "    # Get words that are similar. This returns tuples in a list\n",
    "    # Topn refers to the Top N words. 10 is default\n",
    "    top_n = 5\n",
    "\n",
    "    # Get words that are similar. This returns tuples in a list\n",
    "    similar_words = model.most_similar(word, topn=top_n)\n",
    "\n",
    "    # Calculate the Mean Cosine similarity among words\n",
    "    mean_cos_distance = np.mean([ cos_distance for word, cos_distance in similar_words ])\n",
    "\n",
    "    # Get the collected words that are similar above this score. \n",
    "    # Get the number of words as well\n",
    "    words_above_mean = [word for word, cos_distance in similar_words if cos_distance > mean_cos_distance]\n",
    "    total_words = float(len(words_above_mean))\n",
    "\n",
    "    # Loop over each word\n",
    "    for word in words_above_mean:\n",
    "\n",
    "        # Add the word's vector\n",
    "        avgWordsFeature = np.add(avgWordsFeature,model[word])\n",
    "\n",
    "    # Average them out\n",
    "    avgWordsFeature = np.divide(avgWordsFeature,total_words)\n",
    "\n",
    "    # Return them\n",
    "    return avgWordsFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to transform the data\n",
    "def makeFeatureVec(words, model, vector_dict, num_features):\n",
    "\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "\n",
    "    # Count number of words\n",
    "    nwords = 0.\n",
    "\n",
    "    # Loop over word by word\n",
    "    # If in vocabulary, add its feature vector to the total\n",
    "    for word in words.split():\n",
    "        \n",
    "        if word in model: #and word not in stop_words:\n",
    "            nwords += 1.\n",
    "            avgWordFeature = wordsAverage(word)\n",
    "            featureVec = np.add(featureVec, avgWordFeatureb)\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    \n",
    "    # If number of words zero\n",
    "    if nwords == 0:\n",
    "        featureVec = characterVec(words, model, num_features)\n",
    "    \n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the data. Use the same approach when cleaning reddit data\n",
    "def cleaningSentence(sentence):\n",
    "\n",
    "    #Remove URLs\n",
    "    clean_sentence = re.sub(r'\\w+:\\/\\/\\S+', ' ', sentence)\n",
    "\n",
    "    # Word Standardizing (Ex. Looooolll should be Looll)\n",
    "    clean_sentence = ''.join(''.join(s)[:2] for _, s in itertools.groupby(clean_sentence))\n",
    "\n",
    "    #Convert words to lower case and split them\n",
    "    words = clean_sentence.lower().split()\n",
    "\n",
    "    #Remove contractions by expansion of words\n",
    "    #words = [contractions[word] if word in contractions else word for word in words]\n",
    "\n",
    "    # Rejoin words \n",
    "    words = \" \".join(words)\n",
    "\n",
    "    # Remove non-alphabets\n",
    "    words = re.sub(\"[^a-z\\s]\", \" \", words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sentence to classify: I hope a piece of shit like you die in hell\n",
      "i hope a piece of shit like you die in hell\n"
     ]
    }
   ],
   "source": [
    "sentence = input(\"Enter sentence to classify: \")\n",
    "\n",
    "# Clean the sentence\n",
    "sentence = cleaningSentence(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict using Machine Learning (Use Random Forest)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
