{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Load Word2Vec model\n",
    "from gensim.models import Word2Vec as w2v\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6580"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/clean_dataset.csv\"\n",
    "df = pd.read_csv(FILE)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19286"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the unique words in the dataset\n",
    "unique_words = []\n",
    "\n",
    "for comment in df['Comment']:\n",
    "    \n",
    "    for word in comment.split():\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            \n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE MISSING:  5.916208648760759\n"
     ]
    }
   ],
   "source": [
    "# Check how many are not in the reddit model and collect them\n",
    "not_in_model = []\n",
    "\n",
    "for word in unique_words:\n",
    "    \n",
    "    if word not in model:\n",
    "        not_in_model.append(word)\n",
    "        \n",
    "len(not_in_model)\n",
    "print(\"PERCENTAGE MISSING: \", (len(not_in_model)/len(unique_words)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sort the words then print them. Then, write them in a text file (DONE)\n",
    "# not_in_model = sorted(not_in_model)\n",
    "# with open('Missing words.txt', 'w') as fh:\n",
    "#    for word in not_in_model:\n",
    "#        fh.write(\"{}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146604\n"
     ]
    }
   ],
   "source": [
    "# Get the number of words in the Word2Vec model\n",
    "print(len(model.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arsehole', 0.8830243349075317),\n",
       " ('asshat', 0.8711205124855042),\n",
       " ('prick', 0.8389500975608826),\n",
       " ('dickhead', 0.8373932242393494),\n",
       " ('douchebag', 0.8241000175476074),\n",
       " ('dickbag', 0.8229827880859375),\n",
       " ('ahole', 0.8007912039756775),\n",
       " ('idiot', 0.7848369479179382),\n",
       " ('jackass', 0.7815600037574768),\n",
       " ('douche', 0.7751066088676453)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words that are similar. This returns tuples in a list\n",
    "word = 'asshole'\n",
    "top_n = 10\n",
    "\n",
    "similar_words = model.most_similar(word, topn=top_n)\n",
    "model.most_similar(word, topn=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the Mean Cosine similarity among words\n",
    "import numpy as np\n",
    "\n",
    "mean_cos_distance = np.mean([ cos_distance for word, cos_distance in similar_words ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the collected words that are similar above this score\n",
    "words_above_mean = [word for word, cos_distance in similar_words if cos_distance > mean_cos_distance]\n",
    "total_words = float(len(words_above_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arsehole', 'asshat', 'prick', 'dickhead', 'douchebag', 'dickbag']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_above_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-initialize an empty numpy array (for speed)\n",
    "avgWordsFeature = np.zeros((300,),dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loop over each word\n",
    "for word in words_above_mean:\n",
    "\n",
    "    # Add the word's vector\n",
    "    avgWordsFeature = np.add(avgWordsFeature,model[word])\n",
    "    \n",
    "# Average them out\n",
    "avgWordsFeature = np.divide(avgWordsFeature,total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Use a numpy array to store each word as a feature\n",
    "# Use the first N-words. N is an integer\n",
    "\n",
    "sentenceWordFeature = np.zeros((5,), dtype=\"float32\")\n",
    "print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00470508  0.00158833 -0.00082337  0.          0.        ]\n",
      "[-0.00470508  0.00158833 -0.00082337  0.          0.        ]\n",
      "[-0.00470508  0.00158833 -0.00082337  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Test with a single sentence\n",
    "\n",
    "sentence = \"you goddamn bastard\"\n",
    "complete_sentence = 1 # 1 for full, 0 for incomplete\n",
    "\n",
    "for i,word in enumerate(sentence.split()):\n",
    "    \n",
    "    if i == len(sentenceWordFeature):\n",
    "        complete_sentence = 0\n",
    "        break\n",
    "        \n",
    "    if word in model:\n",
    "        word_feature = np.mean(model[word])\n",
    "    else:\n",
    "        word_feature = -1.0\n",
    "        \n",
    "    sentenceWordFeature[i] = word_feature\n",
    "    \n",
    "    print(sentenceWordFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
