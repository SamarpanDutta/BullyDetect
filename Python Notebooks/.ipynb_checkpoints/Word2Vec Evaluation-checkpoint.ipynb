{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Unigram Testing\n",
    "\n",
    "This Python Notebook is used for evaluation of the Word2Vec Unigram model. The section is broken down as follows:\n",
    "\n",
    "- Find most similar words from the selected word\n",
    "- Perform Syntactic Analysis\n",
    "- Perform Semantic Analysis\n",
    "- Find uncommon word among a list of words\n",
    "- Find cosine similarity among two words\n",
    "- Find the frequency count of a word\n",
    "- Check if a word is in the model\n",
    "- Print preview a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Unigram model\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biopsychology', 0.740115225315094),\n",
       " ('astrochemistry', 0.7391058206558228),\n",
       " ('neuroendocrinologist', 0.7296165227890015),\n",
       " ('nanoscience', 0.7265405058860779),\n",
       " ('neuropharmacology', 0.7247588038444519),\n",
       " ('saltzberg', 0.7157706618309021),\n",
       " ('ethnomusicology', 0.7156946659088135),\n",
       " ('psychobiology', 0.7154250144958496),\n",
       " ('nueroscience', 0.7147186994552612),\n",
       " ('neuropsychiatry', 0.7140935659408569),\n",
       " ('ichthyology', 0.710540235042572),\n",
       " ('molbio', 0.7056220769882202),\n",
       " ('oenology', 0.7056138515472412),\n",
       " ('antropology', 0.7041956186294556),\n",
       " ('biopsych', 0.7037904858589172),\n",
       " ('neuroengineering', 0.7037561535835266),\n",
       " ('nanoengineering', 0.7024978995323181),\n",
       " ('psycholinguistics', 0.7002543210983276),\n",
       " ('bioanthropology', 0.6995773315429688),\n",
       " ('christmann', 0.698868989944458)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to find most similar words \n",
    "# One word for unigram: dragon, bleach, tottenham\n",
    "# Two words for bigram: dragon_ball, barack_obama (UNDERSCORE NEEDED + BIGRAM MODEL LOADED)\n",
    "model.most_similar(\"neuropsychopharmacology\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumpur', 0.6737101674079895),\n",
       " ('kuala', 0.6668090224266052),\n",
       " ('taipei', 0.6401477456092834),\n",
       " ('bangkok', 0.6113026142120361),\n",
       " ('penang', 0.5809809565544128),\n",
       " ('lampur', 0.5752942562103271),\n",
       " ('toyko', 0.5550657510757446),\n",
       " ('selangor', 0.5511509776115417),\n",
       " ('singapore', 0.5502724647521973),\n",
       " ('mumbai', 0.5481346249580383)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for semantic evaluation (Ex. King - man + woman is approximately equal to queen)\n",
    "model.most_similar(positive=[\"tokyo\",\"malaysia\"], negative=[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueish', 0.7298511266708374),\n",
       " ('greyish', 0.7232707738876343),\n",
       " ('bluish', 0.7149738669395447),\n",
       " ('pinkish', 0.705883264541626),\n",
       " ('purplish', 0.7028074264526367),\n",
       " ('brownish', 0.6946163773536682),\n",
       " ('grayish', 0.6922476887702942),\n",
       " ('reddish', 0.6911346316337585),\n",
       " ('yellowish', 0.6770833134651184),\n",
       " ('whitish', 0.6669460535049438)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for syntactic evaluation (Ex. walking - walk + swim is approximately equal to swimming)\n",
    "model.most_similar(positive=[\"greenish\",\"blue\"], negative=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check which word doesn't match among a group of words\n",
    "model.doesnt_match(\"blue green yellow apple\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24046405589195533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check similarity among two words\n",
    "model.similarity(\"titanic\",\"rose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times a specific word occured in the 2015 Dataset\n",
    "word = model.vocab['difu']\n",
    "type(word.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if word (Unigram) is in model. It is case-sensitive\n",
    "'Dragon' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 grafica 928181\n",
      "1 cleanies 311468\n",
      "2 primachs 508246\n",
      "3 cavecricket 435393\n",
      "4 hateri 642158\n",
      "5 waterberry 210428\n",
      "6 zyvox 534227\n",
      "7 conventuals 197106\n",
      "8 jegax 119686\n",
      "9 saroot 423288\n",
      "10 feaiwsy 190247\n",
      "11 usks 1422\n",
      "12 flin 977658\n",
      "13 badgerbadgerbadgerbadger 240399\n",
      "14 yehl 115939\n",
      "15 scammed 1136131\n",
      "16 logially 21020\n",
      "17 cirillas 208087\n",
      "18 carenes 187045\n",
      "19 transdanubia 217067\n",
      "20 ratones 842284\n",
      "21 eixa 666289\n",
      "22 floatstone 920451\n",
      "23 frostproof 653096\n",
      "24 openings 1134513\n",
      "25 salks 584341\n",
      "26 reimers 985084\n",
      "27 orangerx 906499\n",
      "28 redditchella 297205\n",
      "29 belfasties 206136\n",
      "30 guildex 299599\n",
      "31 highsight 953761\n",
      "32 baezult 328718\n",
      "33 gozaru 723128\n",
      "34 posted 1145949\n",
      "35 cirab 213780\n",
      "36 jacoco 488432\n",
      "37 katx 84404\n",
      "38 ducktits 246238\n",
      "39 apartame 477944\n",
      "40 upswung 742037\n",
      "41 twinlink 239292\n",
      "42 desenzano 73410\n",
      "43 ridiciulously 26632\n",
      "44 buildcraft 1097106\n",
      "45 againstgamergate 1046399\n",
      "46 mgoing 709333\n",
      "47 flimbo 252807\n",
      "48 topfragged 994071\n",
      "49 meridethb 6036\n",
      "50 xxyy 843615\n",
      "51 huehuehues 251142\n",
      "52 redidiots 311939\n",
      "53 mandalorians 1089657\n",
      "54 creamsicles 997921\n",
      "55 waiguo 534824\n",
      "56 leanca 659240\n",
      "57 indescrete 557017\n",
      "58 strongerthanyouare 623375\n",
      "59 epaphra 320450\n",
      "60 nonempty 954772\n",
      "61 malapena 44096\n",
      "62 berminverter 375860\n",
      "63 bisexualcuck 187892\n",
      "64 kanagroos 493809\n",
      "65 zeveran 490417\n",
      "66 knollridge 826916\n",
      "67 coolgate 317951\n",
      "68 contrains 448095\n",
      "69 sunrunner 777344\n",
      "70 roadgame 483514\n"
     ]
    }
   ],
   "source": [
    "# A brief review of words in the model\n",
    "count = 70\n",
    "\n",
    "for index, word in enumerate(model.vocab):\n",
    "    print(index, word, model.vocab[word].count)\n",
    "    if index == count:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
