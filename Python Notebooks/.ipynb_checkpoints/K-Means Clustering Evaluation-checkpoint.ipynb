{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Evaluation\n",
    "\n",
    "This Python notebook is used for evaluation of a dictionary that is produced by:\n",
    "\n",
    "- Find the cluster a word belongs to \n",
    "- Find the other words in the cluster for a specific word\n",
    "- Compare between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL WORDS: 1146604 \n",
      "\n",
      "AVERAGE PER CLUSTER (250): 4586\n",
      "AVERAGE PER CLUSTER (500): 2293\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the files\n",
    "FILE_DICT_250 = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/dict_250C.pk\"\n",
    "FILE_CLUS_250 = \"C:/Users/MyPC/Desktop/Vegito/K-Means Models/full_250C.pk\"\n",
    "\n",
    "FILE_DICT_500 = \"C:/Users/MyPC/Desktop/Vegito/Word Dictionaries/dict_500C.pk\"\n",
    "FILE_CLUS_500 = \"C:/Users/MyPC/Desktop/Vegito/K-Means Models/full_500C.pk\"\n",
    "\n",
    "# Load using pickle\n",
    "array_dict_cluster_250 = pickle.load(open(FILE_DICT_250, \"rb\"))\n",
    "word_centroid_map_250 =  pickle.load(open(FILE_CLUS_250,\"rb\"))\n",
    "\n",
    "array_dict_cluster_500 = pickle.load(open(FILE_DICT_500, \"rb\"))\n",
    "word_centroid_map_500 =  pickle.load(open(FILE_CLUS_500,\"rb\"))\n",
    "\n",
    "total_clusters_250 = max(word_centroid_map_250.values()) + 1\n",
    "total_clusters_500 = max(word_centroid_map_500.values()) + 1\n",
    "\n",
    "average_word_250 = round(len(word_centroid_map_250)/total_clusters_250)\n",
    "average_word_500 = round(len(word_centroid_map_500)/total_clusters_500)\n",
    "\n",
    "# Display results\n",
    "print(\"TOTAL WORDS: %i \\n\" % (len(word_centroid_map_250)))\n",
    "\n",
    "print(\"AVERAGE PER CLUSTER (250): %i\" % (average_word_250))\n",
    "print(\"AVERAGE PER CLUSTER (500): %i\" % (average_word_500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCHED WORD: scumbag \n",
      "\n",
      "TOTAL WORDS (250): 3385\n",
      "TOTAL WORDS (500): 1148 \n",
      "\n",
      "WORDS (250):  ['creepster', 'spasticated', 'poseur', 'barrista', 'lyncher', 'assbag', 'douce', 'asahole', 'bitchboy', 'plebeian', 'indulger', 'nutzo', 'schooler', 'sterotype', 'turbonerd', 'showbusiness', 'astroturfer', 'hosebeast', 'fop', 'traitorous', 'trifflin', 'nooblet', 'shithawk', 'shmoe', 'mongoloid', 'interneter', 'ungratefull', 'joykill', 'edumacated', 'toity', 'struggler', 'ludite', 'coldblooded', 'crossfiter', 'scheister', 'softhearted', 'insufferable', 'salty', 'bandwagonner', 'nigged', 'comie', 'plebian', 'bogan', 'mastermind', 'whingey', 'punker', 'scumhole', 'pigkin', 'slaphead', 'schlubby', 'peson', 'bollocking', 'runescaper', 'dindunuffin', 'lapdog', 'fuckpipe', 'meatbag', 'smarmy', 'douchie', 'whore', 'trekkie', 'cockface', 'milquetoast', 'cuntself', 'kneckbeard', 'fag', 'rapie', 'bratty', 'schitck', 'bumptious', 'telemarketer', 'tarded', 'cornball', 'hardon', 'mewling', 'buttheaded', 'scumlord', 'jibbering', 'wierds', 'shrimpdittle', 'champing', 'torpid', 'fuckee', 'fogie', 'narky', 'scumbug', 'derro', 'disgrace', 'babykiller', 'merican', 'goony', 'raconteur', 'goofus', 'douchbag', 'dadbie', 'atics', 'shitbreath', 'assburgers', 'philantropist', 'assmonkey', 'cuckboy', 'skateboarder', 'nitwit', 'germophobe', 'shitfart', 'douchefag', 'gringo', 'snob', 'waahmbulance', 'wierdo', 'chickenhawk', 'toitey', 'cuntflaps', 'sketchball', 'seconder', 'aficionado', 'turdblossom', 'joning', 'jackanapes', 'agitator', 'trekie', 'durianrider', 'spaz', 'techie', 'dickface', 'eurotrash', 'filmer', 'jackwad', 'sissified', 'fearection', 'shtick', 'trendsetter', 'cuntastic', 'meatheaded', 'fatlogician', 'skitzo', 'junkie', 'thuggish', 'dumass', 'filfthy', 'goober', 'poncy', 'mullato', 'quitter', 'bazillionaire', 'sissy', 'cuddler', 'smackhead', 'buzzkill', 'lackie', 'cringelord', 'shitter', 'teenager', 'surfie', 'buttfuck', 'dickwagon', 'memeber', 'doully', 'nympho', 'smacktalking', 'chinless', 'blamer', 'doucehbag', 'speeks', 'looker', 'keystar', 'sconnie', 'gormless', 'cousinfucker', 'idgit', 'cusser', 'hoper', 'clouted', 'rapscallion', 'upitty', 'channer', 'bully', 'waffler', 'sanctimommy', 'twunt', 'pleb', 'illuminutty', 'ohioan', 'dogfucker', 'grunger', 'conniption', 'ameritard', 'uniballer', 'millionare', 'poopoohead', 'stepchild', 'cinephile', 'creepazoid', 'assholr', 'bossy', 'charlottean', 'shitrooster', 'stupis', 'underfucked', 'murkan'] \n",
      "\n",
      "\n",
      "WORDS (500):  ['mong', 'suckup', 'elitest', 'grubber', 'weebo', 'moralfag', 'sucker', 'raver', 'tattle', 'punkass', 'autistic', 'manlet', 'drunkard', 'cockhead', 'pyschopath', 'whitebread', 'coattail', 'arsewipe', 'downer', 'conniption', 'cholo', 'wristed', 'whinger', 'twatbag', 'douce', 'limey', 'pervert', 'retardo', 'coalburner', 'gger', 'windbag', 'genwunner', 'gormless', 'junkie', 'pendantic', 'slanderer', 'stranger', 'nosed', 'snit', 'beaner', 'sellout', 'cuntnugget', 'nutbag', 'spergy', 'wastrel', 'swayer', 'dickass', 'beatnik', 'whinny', 'opportunist', 'botherer', 'weab', 'sleazeball', 'kindergartener', 'pederast', 'pommy', 'hyprocrite', 'simpering', 'lier', 'smarmy', 'crackwhore', 'obeast', 'nebbish', 'douchbag', 'deadbeat', 'sleeze', 'prick', 'turdbucket', 'bookworm', 'madman', 'superfan', 'sonofabitch', 'wackjob', 'pussywhipped', 'hypocrite', 'pisser', 'necrophile', 'dullard', 'dumbfuck', 'scum', 'plebeian', 'niggerfaggot', 'neckbearded', 'whore', 'jackwagon', 'landlubber', 'squeeker', 'skygod', 'lardbeast', 'goof', 'casul', 'bellend', 'fairweather', 'teenybopper', 'shitstirrer', 'tweeker', 'aloner', 'smartarse', 'sexpat', 'fantasist', 'lecher', 'dweeb', 'pretensious', 'sanctimommy', 'chode', 'methed', 'ignoramus', 'baiter', 'unpurple', 'tankie', 'metalhead', 'dingleberry', 'shitbird', 'bawler', 'ghoster', 'sheboon', 'mongoloid', 'extortionist', 'simp', 'hooah', 'alchy', 'bitchly', 'pissant', 'nublet', 'poser', 'dependa', 'kiddy', 'musclehead', 'jerkbag', 'culchie', 'shithole', 'manbaby', 'jigaboo', 'hellhole', 'floozy', 'dopehead', 'egotist', 'ladykiller', 'troller', 'skeeze', 'loudmouthed', 'dicksucker', 'snoozer', 'germophobe', 'druggie', 'tarded', 'weasel', 'brogrammer', 'sod', 'sweetheart', 'dickweed', 'shamer', 'philanderer', 'panderer', 'shitweasel', 'druglord', 'schooler', 'stickler', 'knacker', 'pedant', 'skank', 'dickbag', 'clopper', 'inbred', 'scalper', 'luddite', 'sonuvabitch', 'daywalker', 'cheesedick', 'gymrat', 'redditeur', 'luvvie', 'nigger', 'grubbing', 'scumball', 'normalfag', 'embellisher', 'ingrate', 'chucker', 'faggoty', 'pompous', 'jerkass', 'doucher', 'hippocrite', 'paedo', 'vigin', 'bully', 'braggart', 'wannabe', 'whackjob', 'baffoon', 'kisser', 'lacky', 'friendless', 'grouch', 'groupie', 'hamplanet', 'mooches', 'shitbrick', 'weaboo']\n"
     ]
    }
   ],
   "source": [
    "# Find the cluster of words, based on a given word\n",
    "search = \"scumbag\"\n",
    "\n",
    "# Get the key, or cluster number\n",
    "# NOTE: Different clusters can have same results\n",
    "cluster_num_250 = word_centroid_map_250[search]\n",
    "cluster_num_500 = word_centroid_map_500[search]\n",
    "\n",
    "# Return the array based on the cluster number\n",
    "words_250 = array_dict_cluster_250[cluster_num_250]['word_list']\n",
    "words_500 = array_dict_cluster_500[cluster_num_500]['word_list']\n",
    "\n",
    "# Display results\n",
    "print(\"SEARCHED WORD: %s \\n\" % (search))\n",
    "\n",
    "print(\"TOTAL WORDS (250): %i\" % (len(words_250)))\n",
    "print(\"TOTAL WORDS (500): %i \\n\" % (len(words_500)))\n",
    "\n",
    "print(\"WORDS (250): \", words_250[:200], \"\\n\\n\")\n",
    "print(\"WORDS (500): \", words_500[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# Perform two types of clustering: K-Means and Mini Batch K Means\n",
    "\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING WORD2VEC MODEL \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the Word2Vec model\n",
    "print(\"LOADING WORD2VEC MODEL \\n\\n\")\n",
    "FILE = \"C:/Users/MyPC/Desktop/Vegito/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING WORD VECTORS AND WORDS\n",
      "TRAINING K-MEANS WITH 2000 CLUSTERS \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:1381: RuntimeWarning: init_size=300 should be larger than k=2000. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME TAKEN 284.44727635383606\n",
      "STORING IN DICTIONARY\n"
     ]
    }
   ],
   "source": [
    "WORDS = 100000\n",
    "\n",
    "# Get the word vectors and words\n",
    "print(\"GETTING WORD VECTORS AND WORDS\")\n",
    "word_vectors = model.syn0[:WORDS]\n",
    "words = model.index2word[:WORDS]\n",
    "\n",
    "# Perform Minibatch K-Means clustering\n",
    "# Use 250 Clusters\n",
    "CLUSTERS = 2000\n",
    "k_means = MiniBatchKMeans(n_clusters = CLUSTERS)\n",
    "\n",
    "# Fit the model, get the centroid number and calculate time\n",
    "print(\"TRAINING K-MEANS WITH %i CLUSTERS \\n\\n\" % (CLUSTERS))\n",
    "start = time.time()\n",
    "idx = k_means.fit_predict(word_vectors)\n",
    "end = time.time()\n",
    "\n",
    "print('TIME TAKEN', end-start)\n",
    "\n",
    "# Store it in a dictionary\n",
    "print('STORING IN DICTIONARY')\n",
    "word_centroid_map = dict(zip(words,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER NUMBER: 522\n",
      "NUMBER OF WORDS: 112 \n",
      "\n",
      "WORDS:  ['acapellas', 'grime', 'beatport', 'brostep', 'trappy', 'screamo', 'melodic', 'soundscapes', 'britpop', 'neurofunk', 'electronica', 'breakcore', 'hop', 'oldies', 'nasheeds', 'bap', 'indie', 'idm', 'groovy', 'rnb', 'electronic', 'trance', 'bhangra', 'genres', 'punk', 'psybient', 'dancey', 'jangle', 'bigroom', 'djs', 'subgenre', 'bossa', 'ska', 'djent', 'rockabilly', 'bachata', 'genre', 'eclectic', 'prog', 'psychadelic', 'lofi', 'remixes', 'bluegrass', 'danceable', 'ragga', 'synthpop', 'gabber', 'edm', 'complextro', 'avant', 'psytrance', 'chiptune', 'dubstep', 'funk', 'synthwave', 'nightcore', 'chillwave', 'techno', 'shoegaze', 'synthy', 'hip', 'thrash', 'dnb', 'subgenres', 'mashups', 'grunge', 'remixing', 'moombahton', 'deathcore', 'monstercat', 'jpop', 'chillstep', 'reggae', 'music', 'motown', 'hardstyle', 'dancehall', 'futurebeats', 'riddim', 'mathcore', 'grindcore', 'folky', 'hiphop', 'autotuned', 'psychedelia', 'instrumental', 'reggaeton', 'electroswing', 'mathy', 'chillout', 'chiptunes', 'bluesy', 'metalcore', 'djing', 'melodeath', 'garde', 'jazzy', 'folk', 'tracks', 'vaporwave', 'wubs', 'experimental', 'basshead', 'electro', 'poppier', 'noisey', 'drumstep', 'musics', 'proggy', 'emo', 'downtempo', 'jazz']\n"
     ]
    }
   ],
   "source": [
    "# Test it out\n",
    "word = 'genre'\n",
    "\n",
    "# Get cluster number\n",
    "cluster = word_centroid_map[word]\n",
    "\n",
    "# Append for words in same cluster\n",
    "word_list = [ word for word, cluster_num in word_centroid_map.items() if cluster == cluster_num ]\n",
    "\n",
    "print('CLUSTER NUMBER: %i' % (cluster))\n",
    "print('NUMBER OF WORDS: %i \\n' % (len(word_list)))\n",
    "print('WORDS: ' ,word_list)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
