{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Unigram Testing\n",
    "\n",
    "This Python Notebook is used for evaluation of the Word2Vec Unigram model. The section is broken down as follows:\n",
    "\n",
    "- Find most similar words from the selected word\n",
    "- Perform Syntactic Analysis\n",
    "- Perform Semantic Analysis\n",
    "- Find uncommon word among a list of words\n",
    "- Find cosine similarity among two words\n",
    "- Find the frequency count of a word\n",
    "- Check if a word is in the model\n",
    "- Print preview a list of words\n",
    "- Others (Vector space size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\MyPC\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec as w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Unigram model\n",
    "FILE = \"C:/Users/MyPC/Desktop/FYP/W2V Models/w2v_reddit_unigram_300d.bin\"\n",
    "model = w2v.load_word2vec_format(FILE, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biopsychology', 0.740115225315094),\n",
       " ('astrochemistry', 0.7391058206558228),\n",
       " ('neuroendocrinologist', 0.7296165227890015),\n",
       " ('nanoscience', 0.7265405058860779),\n",
       " ('neuropharmacology', 0.7247588038444519),\n",
       " ('saltzberg', 0.7157706618309021),\n",
       " ('ethnomusicology', 0.7156946659088135),\n",
       " ('psychobiology', 0.7154250144958496),\n",
       " ('nueroscience', 0.7147186994552612),\n",
       " ('neuropsychiatry', 0.7140935659408569),\n",
       " ('ichthyology', 0.710540235042572),\n",
       " ('molbio', 0.7056220769882202),\n",
       " ('oenology', 0.7056138515472412),\n",
       " ('antropology', 0.7041956186294556),\n",
       " ('biopsych', 0.7037904858589172),\n",
       " ('neuroengineering', 0.7037561535835266),\n",
       " ('nanoengineering', 0.7024978995323181),\n",
       " ('psycholinguistics', 0.7002543210983276),\n",
       " ('bioanthropology', 0.6995773315429688),\n",
       " ('christmann', 0.698868989944458)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to find most similar words \n",
    "# One word for unigram: dragon, bleach, tottenham\n",
    "# Two words for bigram: dragon_ball, barack_obama (UNDERSCORE NEEDED + BIGRAM MODEL LOADED)\n",
    "model.most_similar(\"neuropsychopharmacology\", topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lumpur', 0.6737101674079895),\n",
       " ('kuala', 0.6668090224266052),\n",
       " ('taipei', 0.6401477456092834),\n",
       " ('bangkok', 0.6113026142120361),\n",
       " ('penang', 0.5809809565544128),\n",
       " ('lampur', 0.5752942562103271),\n",
       " ('toyko', 0.5550657510757446),\n",
       " ('selangor', 0.5511509776115417),\n",
       " ('singapore', 0.5502724647521973),\n",
       " ('mumbai', 0.5481346249580383)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for semantic evaluation (Ex. King - man + woman is approximately equal to queen)\n",
    "model.most_similar(positive=[\"tokyo\",\"malaysia\"], negative=[\"japan\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueish', 0.7298511266708374),\n",
       " ('greyish', 0.7232707738876343),\n",
       " ('bluish', 0.7149738669395447),\n",
       " ('pinkish', 0.705883264541626),\n",
       " ('purplish', 0.7028074264526367),\n",
       " ('brownish', 0.6946163773536682),\n",
       " ('grayish', 0.6922476887702942),\n",
       " ('reddish', 0.6911346316337585),\n",
       " ('yellowish', 0.6770833134651184),\n",
       " ('whitish', 0.6669460535049438)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell for syntactic evaluation (Ex. walking - walk + swim is approximately equal to swimming)\n",
    "model.most_similar(positive=[\"greenish\",\"blue\"], negative=[\"green\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check which word doesn't match among a group of words\n",
    "model.doesnt_match(\"blue green yellow apple\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24046405589195533"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell to check similarity among two words\n",
    "model.similarity(\"titanic\",\"rose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of times a specific word occured in the 2015 Dataset\n",
    "word = model.vocab['difu']\n",
    "type(word.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if word (Unigram) is in model. It is case-sensitive\n",
    "'Dragon' in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 durantula\n",
      "1 difu\n",
      "2 mackinack\n",
      "3 sview\n",
      "4 illid\n",
      "5 volatage\n",
      "6 kangarooland\n",
      "7 factless\n",
      "8 respondan\n",
      "9 wolfphram\n",
      "10 siedrah\n",
      "11 furbutt\n",
      "12 svbtle\n",
      "13 mouskeys\n",
      "14 divines\n",
      "15 flawlessfakeids\n",
      "16 treeborn\n",
      "17 conmercial\n",
      "18 jagaloon\n",
      "19 berusaha\n",
      "20 godmen\n",
      "21 errico\n",
      "22 auncle\n",
      "23 civitates\n",
      "24 nintendan\n",
      "25 limnologist\n",
      "26 sterilizes\n",
      "27 adamvideo\n",
      "28 guaifenesin\n",
      "29 qzcz\n",
      "30 ixnay\n",
      "31 laurinatis\n",
      "32 allora\n",
      "33 publisize\n",
      "34 konspirazytheory\n",
      "35 mol\n",
      "36 unproportionately\n",
      "37 audiobookbay\n",
      "38 tarutaru\n",
      "39 autocombos\n",
      "40 crimebusters\n",
      "41 fauhawks\n",
      "42 franjo\n",
      "43 makovsky\n",
      "44 kirkle\n",
      "45 bater\n",
      "46 leavins\n",
      "47 vilely\n",
      "48 manina\n",
      "49 jemmie\n",
      "50 ruhkmar\n",
      "51 musiczlife\n",
      "52 preciptated\n",
      "53 grillmaster\n",
      "54 animate\n",
      "55 saluzzo\n",
      "56 afuckin\n",
      "57 dimentions\n",
      "58 recharger\n",
      "59 ygfny\n",
      "60 garnagle\n",
      "61 brickoftr\n",
      "62 knotting\n",
      "63 weaned\n",
      "64 zhills\n",
      "65 metalls\n",
      "66 omanomanoman\n",
      "67 footbar\n",
      "68 accentual\n",
      "69 baesball\n",
      "70 basetypes\n"
     ]
    }
   ],
   "source": [
    "# A brief review of words in the model\n",
    "count = 70\n",
    "\n",
    "for index, word in enumerate(model.vocab):\n",
    "    print(index, word)\n",
    "    if index == count:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143362"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking under the hood of Word2vec\n",
    "\n",
    "# Feature vector for each word(s) \n",
    "#print(model.syn0[0])\n",
    "\n",
    "# Shape of the vocabulary\n",
    "#print(model.syn0.shape)\n",
    "\n",
    "# Get list of all keys (words)\n",
    "#model.index2word[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
